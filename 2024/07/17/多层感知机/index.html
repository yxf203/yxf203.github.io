<!doctype html>
<html lang="zh"><head>
<title>多层感知机 - fff从零开始的博客OvO</title>
<meta charset="UTF-8">
<meta name="keywords" content="">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=5">

<link rel="shortcut icon" href="/favicon.ico" type="image/x-icon" />
<meta name="description" content="多层感知机  隐藏层   线性模型可能会出错（毕竟以像素为例，像素的重要性都以复杂的方式取决于该像素的上下文）。而我们的数据可能会有一种表示，这种表示会考虑到我们在特征之间的相关交互作用。在此表示的基础上建立一个线性模型可能会是合适的，但我们不知道如何手动计算这么一种表示。对于深度神经网络，我们使用观测数据来联合学习隐藏层表示和应用于该表示的线性预测器。   我们可以通过在网络中加入一个或多个隐">
<meta property="og:type" content="article">
<meta property="og:title" content="多层感知机">
<meta property="og:url" content="https://yxf203.github.io/2024/07/17/%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA/index.html">
<meta property="og:site_name" content="fff从零开始的博客OvO">
<meta property="og:description" content="多层感知机  隐藏层   线性模型可能会出错（毕竟以像素为例，像素的重要性都以复杂的方式取决于该像素的上下文）。而我们的数据可能会有一种表示，这种表示会考虑到我们在特征之间的相关交互作用。在此表示的基础上建立一个线性模型可能会是合适的，但我们不知道如何手动计算这么一种表示。对于深度神经网络，我们使用观测数据来联合学习隐藏层表示和应用于该表示的线性预测器。   我们可以通过在网络中加入一个或多个隐">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://yxf203.github.io/2024/07/17/images/posts/MLP/hiddenlayer.png">
<meta property="og:image" content="https://yxf203.github.io/2024/07/17/images/posts/MLP/normfitting.png">
<meta property="og:image" content="https://yxf203.github.io/2024/07/17/images/posts/MLP/underfitting.png">
<meta property="og:image" content="https://yxf203.github.io/2024/07/17/images/posts/MLP/overfitting.png">
<meta property="og:image" content="https://yxf203.github.io/2024/07/17/images/posts/MLP/ignore.png">
<meta property="og:image" content="https://yxf203.github.io/2024/07/17/images/posts/MLP/reduction.png">
<meta property="article:published_time" content="2024-07-17T11:09:07.000Z">
<meta property="article:modified_time" content="2024-07-17T11:09:07.183Z">
<meta property="article:author" content="YXF">
<meta property="article:tag" content="动手学深度学习">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://yxf203.github.io/2024/07/17/images/posts/MLP/hiddenlayer.png">

<link rel="stylesheet" href="/lib/fancybox/fancybox.css">
<link rel="stylesheet" href="/lib/mdui_043tiny/mdui.css">


<link rel="stylesheet" href="/lib/iconfont/iconfont.css?v=1721214559380">

<link rel="stylesheet" href="/css/style.css?v=1721214559380">




    
        <link rel="stylesheet" href="/custom.css?v=1721214559380">
    



<script src="/lib/mdui_043tiny/mdui.js" async></script>
<script src="/lib/fancybox/fancybox.umd.js" async></script>
<script src="/lib/lax.min.js" async></script>


<script async src="/js/app.js?v=1721214559380"></script>

 

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-4D4ZJ9G024"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag("js", new Date());

  gtag("config", "G-4D4ZJ9G024");
</script>


<link rel="stylesheet"  href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/atom-one-dark.min.css">
<meta name="generator" content="Hexo 6.3.0"></head><body class="nexmoe mdui-drawer-body-left"><div id="nexmoe-background"><div class="nexmoe-bg" style="background-image: url(/../../images/bcg.png)"></div><div class="mdui-appbar mdui-shadow-0"><div class="mdui-toolbar"><a class="mdui-btn mdui-btn-icon mdui-ripple" mdui-drawer="{target: &#039;#drawer&#039;, swipe: true}" title="menu"><i class="mdui-icon nexmoefont icon-menu"></i></a><div class="mdui-toolbar-spacer"></div><a class="mdui-btn mdui-btn-icon" href="/" title="YXF"><img src="/../../images/avatar.jpg" alt="YXF"></a></div></div></div><div id="nexmoe-header"><div class="nexmoe-drawer mdui-drawer" id="drawer">
    <div class="nexmoe-avatar mdui-ripple">
        <a href="/" title="YXF">
            <img src="/../../images/avatar.jpg" alt="YXF" alt="YXF">
        </a>
    </div>
    <div class="nexmoe-count">
        <div><span>文章</span>16</div>
        <div><span>标签</span>10</div>
        <div><span>分类</span>4</div>
    </div>
    <div class="nexmoe-list mdui-list" mdui-collapse="{accordion: true}">
        
        <a class="nexmoe-list-item mdui-list-item mdui-ripple false" href="/" title="回到首页">
            <i class="mdui-list-item-icon nexmoefont icon-home"></i>
            <div class="mdui-list-item-content">
                回到首页
            </div>
        </a>
        
        <a class="nexmoe-list-item mdui-list-item mdui-ripple false" href="/archive.html" title="文章归档">
            <i class="mdui-list-item-icon nexmoefont icon-container"></i>
            <div class="mdui-list-item-content">
                文章归档
            </div>
        </a>
        
    </div>
    
    
        
        
        
        
        
  <div class="nexmoe-widget-wrap">
    <h3 class="nexmoe-widget-title">文章归档</h3>
    <div class="nexmoe-widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/">2024</a><span class="archive-list-count">16</span></li></ul>
    </div>
  </div>



    
        
        
  <div class="nexmoe-widget-wrap">
    <h3 class="nexmoe-widget-title">最新文章</h3>
    <div class="nexmoe-widget">
      <ul>
        
          <li>
            <a href="/2024/07/17/%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA/">多层感知机</a>
          </li>
        
          <li>
            <a href="/2024/07/15/%E7%BA%BF%E6%80%A7%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/">线性神经网络</a>
          </li>
        
          <li>
            <a href="/2024/07/13/%E9%A2%84%E5%A4%87%E7%9F%A5%E8%AF%86/">预备知识</a>
          </li>
        
          <li>
            <a href="/2024/07/11/%E5%BC%95%E8%A8%80/">引言</a>
          </li>
        
          <li>
            <a href="/2024/07/11/%E6%82%AC%E7%BA%BF%E6%B3%95/">悬线法</a>
          </li>
        
      </ul>
    </div>
  </div>

    
   
    <div class="nexmoe-copyright">
        &copy; 2024 YXF
        Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
        & <a href="https://github.com/theme-nexmoe/hexo-theme-nexmoe" target="_blank">Nexmoe</a>
        
    </div>
</div><!-- .nexmoe-drawer --></div><div id="nexmoe-content"><div class="nexmoe-primary"><div class="nexmoe-post">
  <article>
    
        <div class="nexmoe-post-cover"> 
            <img src="/../../images/bcg.png" alt="多层感知机" loading="lazy">
            <h1>多层感知机</h1>
        </div>
    
    
    <div class="nexmoe-post-meta">
    <div class="nexmoe-rainbow">
        <a class="nexmoefont icon-calendar-fill">2024年07月17日</a>
        
            <a class="nexmoefont icon-appstore-fill -link" href="/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">学习笔记</a>
        
        
    </div>
    
    
    
    
    
</div>

    <h2 id="多层感知机"><a class="markdownIt-Anchor" href="#多层感知机"></a> 多层感知机</h2>
<h3 id="隐藏层"><a class="markdownIt-Anchor" href="#隐藏层"></a> 隐藏层</h3>
<ul>
<li>
<p>线性模型可能会出错（毕竟以像素为例，像素的重要性都以复杂的方式取决于该像素的上下文）。而我们的数据可能会有一种表示，这种表示会考虑到我们在特征之间的相关交互作用。在此表示的基础上建立一个线性模型可能会是合适的，但我们不知道如何手动计算这么一种表示。对于深度神经网络，我们使用观测数据来联合学习隐藏层表示和应用于该表示的<strong>线性预测器</strong>。</p>
</li>
<li>
<p>我们可以通过在网络中加入一个或多个隐藏层来克服线性模型的限制，使其能处理更普遍的函数关系类型。要做到这一点，最简单的方法是<strong>将许多全连接层堆叠在一起。每一层都输出到上面的层，直到生成最后的输出。</strong></p>
</li>
<li>
<p>把前<em>L−1</em>层看作表示，把最后一层看作线性预测器。这种架构通常称为多层感知机（multilayer perceptron），通常缩写为<em>MLP</em>。</p>
</li>
</ul>
<p><img onerror="imgOnError(this);" data-fancybox="gallery" src="../images/posts/MLP/hiddenlayer.png" alt="hiddenlayer" data-caption="hiddenlayer" loading="lazy"></p>
<ul>
<li>
<p>如果单单只是依旧保留隐藏层是<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">H</mi><mo>=</mo><msup><mrow><mi mathvariant="bold">X</mi><mi mathvariant="bold">W</mi></mrow><mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msup><mo>+</mo><msup><mi mathvariant="bold">b</mi><mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msup></mrow><annotation encoding="application/x-tex">\mathbf H = \mathbf{XW}^{(1)}+\mathbf b^{(1)}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68611em;vertical-align:0em;"></span><span class="mord mathbf">H</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.04734em;vertical-align:-0.08333em;"></span><span class="mord"><span class="mord"><span class="mord mathbf">X</span><span class="mord mathbf" style="margin-right:0.01597em;">W</span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.96401em;"><span style="top:-3.13901em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mtight">1</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.8879999999999999em;vertical-align:0em;"></span><span class="mord"><span class="mord mathbf">b</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8879999999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mtight">1</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span></span></span></span>​的形式，那么仿射函数的仿射函数本身就是仿射函数，这么做是没有意义的。因此，为了发挥多层架构的潜力，我们还需要一个额外的关键要素：在仿射变换之后对每个隐藏单元应用非线性的激活函数（activation function）<em>σ</em>。激活函数的输出（例如，σ(·)）被称为活性值（activations）。</p>
<p class='katex-block'><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi mathvariant="bold">H</mi><mo>=</mo><mi>σ</mi><mo stretchy="false">(</mo><msup><mrow><mi mathvariant="bold">X</mi><mi mathvariant="bold">W</mi></mrow><mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msup><mo>+</mo><msup><mi mathvariant="bold">b</mi><mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msup><mo stretchy="false">)</mo><mo separator="true">,</mo><mspace linebreak="newline"></mspace><mi mathvariant="bold">O</mi><mo>=</mo><msup><mrow><mi mathvariant="bold">H</mi><mi mathvariant="bold">W</mi></mrow><mrow><mo stretchy="false">(</mo><mn>2</mn><mo stretchy="false">)</mo></mrow></msup><mo>+</mo><msup><mi mathvariant="bold">b</mi><mrow><mo stretchy="false">(</mo><mn>2</mn><mo stretchy="false">)</mo></mrow></msup><mi mathvariant="normal">.</mi><mspace linebreak="newline"></mspace></mrow><annotation encoding="application/x-tex">\mathbf H = \sigma(\mathbf{XW}^{(1)}+\mathbf b^{(1)}), \\
\mathbf O = \mathbf{HW}^{(2)}+\mathbf b^{(2)}. \\
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68611em;vertical-align:0em;"></span><span class="mord mathbf">H</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.21401em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">σ</span><span class="mopen">(</span><span class="mord"><span class="mord"><span class="mord mathbf">X</span><span class="mord mathbf" style="margin-right:0.01597em;">W</span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.96401em;"><span style="top:-3.13901em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mtight">1</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1.188em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathbf">b</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.938em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mtight">1</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span><span class="mclose">)</span><span class="mpunct">,</span></span><span class="mspace newline"></span><span class="base"><span class="strut" style="height:0.68611em;vertical-align:0em;"></span><span class="mord mathbf">O</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.04734em;vertical-align:-0.08333em;"></span><span class="mord"><span class="mord"><span class="mord mathbf">H</span><span class="mord mathbf" style="margin-right:0.01597em;">W</span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.96401em;"><span style="top:-3.13901em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mtight">2</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.938em;vertical-align:0em;"></span><span class="mord"><span class="mord mathbf">b</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.938em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mtight">2</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span><span class="mord">.</span></span><span class="mspace newline"></span></span></span></span></p>
<p>加入了激活函数，我们的多层感知机就不会退化成线性模型。</p>
</li>
<li>
<p>多层感知机可以通过隐藏神经元，捕捉到输入之间复杂的相互作用，这些神经元依赖于每个输入的值。我们可以很容易地设计隐藏节点来执行任意计算</p>
</li>
</ul>
<h3 id="激活函数"><a class="markdownIt-Anchor" href="#激活函数"></a> 激活函数</h3>
<h4 id="relu函数rectified-linear-unit"><a class="markdownIt-Anchor" href="#relu函数rectified-linear-unit"></a> ReLU函数（Rectified linear unit）</h4>
<ul>
<li>
<p>因为它实现简单，同时在各种预测任务中表现良好。</p>
</li>
<li>
<p>给定元素x，ReLU函数被定义为该元素与0的最大值：</p>
<p class='katex-block'><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>R</mi><mi>e</mi><mi>L</mi><mi>U</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mi>m</mi><mi>a</mi><mi>x</mi><mo stretchy="false">(</mo><mi>x</mi><mo separator="true">,</mo><mn>0</mn><mo stretchy="false">)</mo><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">ReLU(x) = max(x,0).
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.00773em;">R</span><span class="mord mathnormal">e</span><span class="mord mathnormal">L</span><span class="mord mathnormal" style="margin-right:0.10903em;">U</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">m</span><span class="mord mathnormal">a</span><span class="mord mathnormal">x</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord">0</span><span class="mclose">)</span><span class="mord">.</span></span></span></span></span></p>
</li>
<li>
<p>ReLU函数通过将相应的活性值设为0，仅保留正元素并丢弃所有负元素。</p>
</li>
<li>
<p>当输入为负时，ReLU函数的导数为0，而当输入为正时，ReLU函数的导数为1。</p>
</li>
<li>
<p>使用ReLU的原因是，它<strong>求导表现得特别好</strong>：要么让参数消失，要么让参数通过。这使得优化表现得更好，并且ReLU减轻了困扰以往神经网络的梯度消失问题.</p>
<ul>
<li>梯度消失问题是什么？！</li>
</ul>
</li>
<li>
<p>ReLU函数的变体：</p>
<ul>
<li>参数化ReLU（Parameterized ReLU，pReLU）函数 (He et al., 2015)。该变体为ReLU添加了一个线性项，因此即使参数是负的，某些信息仍然可以通过：<p class='katex-block'><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>p</mi><mi>R</mi><mi>e</mi><mi>L</mi><mi>U</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mi>m</mi><mi>a</mi><mi>x</mi><mo stretchy="false">(</mo><mn>0</mn><mo separator="true">,</mo><mi>x</mi><mo stretchy="false">)</mo><mo>+</mo><mi>α</mi><mi>m</mi><mi>i</mi><mi>n</mi><mo stretchy="false">(</mo><mn>0</mn><mo separator="true">,</mo><mi>x</mi><mo stretchy="false">)</mo><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">pReLU(x) = max(0, x) + \alpha min(0,x).
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">p</span><span class="mord mathnormal" style="margin-right:0.00773em;">R</span><span class="mord mathnormal">e</span><span class="mord mathnormal">L</span><span class="mord mathnormal" style="margin-right:0.10903em;">U</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">m</span><span class="mord mathnormal">a</span><span class="mord mathnormal">x</span><span class="mopen">(</span><span class="mord">0</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.0037em;">α</span><span class="mord mathnormal">m</span><span class="mord mathnormal">i</span><span class="mord mathnormal">n</span><span class="mopen">(</span><span class="mord">0</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mord">.</span></span></span></span></span></p>
</li>
</ul>
</li>
</ul>
<h4 id="sigmoid函数"><a class="markdownIt-Anchor" href="#sigmoid函数"></a> sigmoid函数</h4>
<ul>
<li>
<p>对于一个定义域在R中的输入，sigmoid函数将输入变换为区间(0, 1)上的输出。因此被称为挤压函数(squashing function)</p>
<p class='katex-block'><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>s</mi><mi>i</mi><mi>g</mi><mi>m</mi><mi>o</mi><mi>i</mi><mi>d</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mfrac><mn>1</mn><mrow><mn>1</mn><mo>+</mo><msup><mi>e</mi><mrow><mo>−</mo><mi>x</mi></mrow></msup></mrow></mfrac></mrow><annotation encoding="application/x-tex">sigmoid(x) = \frac{1}{1+e^{-x}}
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">s</span><span class="mord mathnormal">i</span><span class="mord mathnormal" style="margin-right:0.03588em;">g</span><span class="mord mathnormal">m</span><span class="mord mathnormal">o</span><span class="mord mathnormal">i</span><span class="mord mathnormal">d</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:2.09077em;vertical-align:-0.7693300000000001em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.32144em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">1</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord"><span class="mord mathnormal">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.697331em;"><span style="top:-2.989em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mathnormal mtight">x</span></span></span></span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.7693300000000001em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span></p>
</li>
<li>
<p>是一个平滑的、可微的阈值单元近似。</p>
</li>
</ul>
<h4 id="tanh函数"><a class="markdownIt-Anchor" href="#tanh函数"></a> tanh函数</h4>
<ul>
<li>与sigmoid函数类似，tanh(双曲正切)函数也能将其输入压缩转换到区间(‐1, 1)上。tanh函数的公式如下：<p class='katex-block'><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>t</mi><mi>a</mi><mi>n</mi><mi>h</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mfrac><mrow><mn>1</mn><mo>−</mo><msup><mi>e</mi><mrow><mo>−</mo><mn>2</mn><mi>x</mi></mrow></msup></mrow><mrow><mn>1</mn><mo>+</mo><msup><mi>e</mi><mrow><mo>−</mo><mn>2</mn><mi>x</mi></mrow></msup></mrow></mfrac></mrow><annotation encoding="application/x-tex">tan h(x)=\frac{1-e^{-2x}}{1+e^{-2x}}
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">t</span><span class="mord mathnormal">a</span><span class="mord mathnormal">n</span><span class="mord mathnormal">h</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:2.260438em;vertical-align:-0.7693300000000001em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.491108em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">1</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord"><span class="mord mathnormal">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.740108em;"><span style="top:-2.989em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mtight">2</span><span class="mord mathnormal mtight">x</span></span></span></span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">1</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord"><span class="mord mathnormal">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mtight">2</span><span class="mord mathnormal mtight">x</span></span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.7693300000000001em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span></p>
</li>
</ul>
<h2 id="多层感知机的从零开始实现"><a class="markdownIt-Anchor" href="#多层感知机的从零开始实现"></a> 多层感知机的从零开始实现</h2>
<h3 id="读取数据和初始化模型参数"><a class="markdownIt-Anchor" href="#读取数据和初始化模型参数"></a> 读取数据和初始化模型参数</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn<br><span class="hljs-keyword">from</span> d2l <span class="hljs-keyword">import</span> torch <span class="hljs-keyword">as</span> d2l<br>batch_size = <span class="hljs-number">256</span><br>train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)<br><span class="hljs-comment"># 初始化模型参数</span><br>num_inputs, num_outputs, num_hiddens = <span class="hljs-number">784</span>, <span class="hljs-number">10</span>, <span class="hljs-number">256</span><br>W1 = nn.Parameter(torch.randn(<br>	num_inputs, num_hiddens, requires_grad=<span class="hljs-literal">True</span>) * <span class="hljs-number">0.01</span>)<br>b1 = nn.Parameter(torch.zeros(num_hiddens, requires_grad=<span class="hljs-literal">True</span>))<br>W2 = nn.Parameter(torch.randn(<br>	num_hiddens, num_outputs, requires_grad=<span class="hljs-literal">True</span>) * <span class="hljs-number">0.01</span>)<br>b2 = nn.Parameter(torch.zeros(num_outputs, requires_grad=<span class="hljs-literal">True</span>))<br>params = [W1, b1, W2, b2]<br></code></pre></td></tr></table></figure>
<ul>
<li>这里设置了256个隐藏单元，通常，我们选择2的若干次幂作为层的宽度。因为内存在硬件中的分配和寻址方式，这么做往往可以在计算上更高效。</li>
<li>我们用几个张量来表示我们的参数。注意，对于每一层我们都要记录<strong>一个权重矩阵和一个偏置向量</strong>。跟以前一样，我们要<strong>为损失关于这些参数的梯度分配内存</strong>。</li>
</ul>
<h3 id="激活函数-2"><a class="markdownIt-Anchor" href="#激活函数-2"></a> 激活函数</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 手动实现ReLU激活函数</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">relu</span>(<span class="hljs-params">X</span>):<br>    a = torch.zeros_like(X)<br>    <span class="hljs-keyword">return</span> torch.<span class="hljs-built_in">max</span>(X, a)<br></code></pre></td></tr></table></figure>
<h3 id="模型"><a class="markdownIt-Anchor" href="#模型"></a> 模型</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 模型</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">net</span>(<span class="hljs-params">X</span>):<br>    X = X.reshape((-<span class="hljs-number">1</span>, num_inputs))<br>    H = relu(X@W1 + b1) <span class="hljs-comment"># 这里“@”代表矩阵乘法</span><br>    <span class="hljs-keyword">return</span> (H@W2 + b2)<br></code></pre></td></tr></table></figure>
<ul>
<li>
<p>这里@的作用是什么？</p>
<ul>
<li>在 Python 中，符号 <code>@</code> 是用于矩阵乘法的运算符。</li>
</ul>
</li>
<li>
<p>这个模型就是先进行了一次输入层到隐藏层的计算得到H，然后再从隐藏层到输出层，得到最后的返回值。</p>
</li>
</ul>
<h3 id="损失函数与训练"><a class="markdownIt-Anchor" href="#损失函数与训练"></a> 损失函数与训练</h3>
<ul>
<li>MLP的训练过程与softmax回归的训练过程完全相同。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 损失函数</span><br>loss = nn.CrossEntropyLoss(reduction=<span class="hljs-string">&#x27;none&#x27;</span>)<br><span class="hljs-comment"># 训练</span><br>num_epochs, lr = <span class="hljs-number">10</span>, <span class="hljs-number">0.1</span><br><span class="hljs-comment"># 选择随机梯度下降法更新参数</span><br>updater = torch.optim.SGD(params, lr=lr)<br>d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, updater)<br>d2l.predict_ch3(net, test_iter)<br>d2l.plt.show()<br></code></pre></td></tr></table></figure>
<h2 id="多层感知机的简洁实现"><a class="markdownIt-Anchor" href="#多层感知机的简洁实现"></a> 多层感知机的简洁实现</h2>
<ul>
<li>与softmax回归的简洁实现相比，其实只有net的部分发生了改变。
<ul>
<li>softmax回归中只有展平层和输出层（只有一个全连接层）。</li>
<li>而这里我们有两个全连接层，即隐藏层和输出层。</li>
<li>便于理解，可以看成是展平层-&gt;隐藏层-&gt;激活层-&gt;输出层。这样也符合<code>sequential</code>容器本身的含义，也容易在脑中形成图像。</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn<br><span class="hljs-keyword">from</span> d2l <span class="hljs-keyword">import</span> torch <span class="hljs-keyword">as</span> d2l<br><br>net = nn.Sequential(nn.Flatten(),<br>nn.Linear(<span class="hljs-number">784</span>, <span class="hljs-number">256</span>),<br>nn.ReLU(),<br>nn.Linear(<span class="hljs-number">256</span>, <span class="hljs-number">10</span>))<br><span class="hljs-keyword">def</span> <span class="hljs-title function_">init_weights</span>(<span class="hljs-params">m</span>):<br>    <span class="hljs-keyword">if</span> <span class="hljs-built_in">type</span>(m) == nn.Linear:<br>        nn.init.normal_(m.weight, std=<span class="hljs-number">0.01</span>)<br>net.apply(init_weights)<br><br>batch_size, lr, num_epochs = <span class="hljs-number">256</span>, <span class="hljs-number">0.1</span>, <span class="hljs-number">10</span><br>loss = nn.CrossEntropyLoss(reduction=<span class="hljs-string">&#x27;none&#x27;</span>)<br>trainer = torch.optim.SGD(net.parameters(), lr=lr)<br>train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)<br>d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)<br></code></pre></td></tr></table></figure>
<h2 id="模型选择-欠拟合和过拟合"><a class="markdownIt-Anchor" href="#模型选择-欠拟合和过拟合"></a> 模型选择、欠拟合和过拟合</h2>
<ul>
<li>我们的目标是发现某些模式，这些模式捕捉到了我们训练集潜在总体的规律，从而达到能够泛化的目的。</li>
<li>将模型在训练数据上拟合的比在潜在分布中更接近的现象称为<strong>过拟合（overfitting）</strong>，用于对抗过拟合的技术称为<strong>正则化（regularization）</strong>。</li>
</ul>
<h3 id="训练误差和泛化误差"><a class="markdownIt-Anchor" href="#训练误差和泛化误差"></a> 训练误差和泛化误差</h3>
<ul>
<li>训练误差（training error）：模型在<strong>训练数据集</strong>上计算得到的误差。</li>
<li>泛化误差（generalization error）：模型应用在同样从<strong>原始样本</strong>的分布中抽取的无限多数据样本时，<strong>模型误差的期望</strong>。（但是因为<em>无限多</em>是一个虚构的对象，泛化误差无法准确测出。实验中往往使用测试集——随机选取的、未曾在训练集中出现的数据样本。）</li>
<li>倾向于影响模型泛化的因素：
<ul>
<li><strong>可调整参数的数量</strong>。当可调整参数的数量（有时称为自由度）很大时，模型往往更容易过拟合。</li>
</ul>
<ol start="2">
<li><strong>参数采用的值</strong>。当权重的取值范围较大时，模型可能更容易过拟合。</li>
</ol>
<ul>
<li><strong>训练样本的数量</strong>。即使模型很简单，也很容易过拟合只包含一两个样本的数据集。而过拟合一个有百万个样本的数据集则需要一个极其灵活的模型。</li>
</ul>
</li>
</ul>
<h3 id="模型选择"><a class="markdownIt-Anchor" href="#模型选择"></a> 模型选择</h3>
<ul>
<li>
<p>模型选择：评估几个候选模型后选择最终的模型 或者 比较不同的超参数设置下的同一类模型。</p>
</li>
<li>
<p>例如，训练多层感知机模型时，我们可能希望比较具有不同数量的隐藏层、不同数量的隐藏单元以及不同的激活函数组合的模型。为了确定候选模型中的最佳模型，我们通常会使用验证集。</p>
</li>
</ul>
<h4 id="验证集"><a class="markdownIt-Anchor" href="#验证集"></a> 验证集</h4>
<ul>
<li>
<p>因为现实实验中不一定能有充足的数据使每次实验都能有全新的数据集，但是拿数据集来选择模型，容易导致过拟合了测试数据。解决此问题的常见做法是将我们的数据分成三份，除了<strong>训练</strong>和<strong>测试</strong>数据集之外，还增加一个<strong>验证</strong>数据集（validation dataset），也叫验证集（validation set）。</p>
</li>
<li>
<p><em>K</em>折交叉验证</p>
<ul>
<li>当<strong>训练数据稀缺</strong>时，我们甚至可能无法提供足够的数据来构成一个合适的验证集。这个问题的一个流行的解决方案是采用<strong><em>K</em>折交叉验证</strong>。这里，原始训练数据被分成<em>K</em>个不重叠的子集。然后执行<em>K</em>次模型训练和验证，每次在<em>K</em> <em>−</em> 1个子集上进行训练，并在剩余的一个子集（在该轮中没有用于训练的子集）上进行验证。最后，通过对<em>K</em>次实验的结果取平均来估计训练和验证误差。</li>
</ul>
</li>
</ul>
<h4 id="欠拟合还是过拟合"><a class="markdownIt-Anchor" href="#欠拟合还是过拟合"></a> 欠拟合还是过拟合？</h4>
<ul>
<li>
<p>训练误差和泛化误差都很严重，但二者差距较笑哦，说明模型过于简单（表达能力不足），我们可以相信，我们可以用一个更复杂的模型降低训练误差。这种情况叫<strong>欠拟合(underfitting)</strong>。</p>
</li>
<li>
<p>我们的训练误差明显低于验证误差时要小心，这表明严重的<strong>过拟合（overfitting）</strong>。</p>
</li>
<li>
<p>欠拟合是指模型无法继续减少训练误差。过拟合是指训练误差远小于验证误差。</p>
</li>
<li>
<p>欠拟合或者过拟合受<strong>模型复杂度</strong>和<strong>数据集大小</strong>（训练数据集中的样本越少，我们就越有可能（且更严重地）过拟合。随着训练数据量的增加，泛化误差通常会减小。）的影响。</p>
</li>
</ul>
<h4 id="多项式回归"><a class="markdownIt-Anchor" href="#多项式回归"></a> 多项式回归</h4>
<h5 id="生成数据"><a class="markdownIt-Anchor" href="#生成数据"></a> 生成数据</h5>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> math<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn<br><span class="hljs-keyword">from</span> d2l <span class="hljs-keyword">import</span> torch <span class="hljs-keyword">as</span> d2l<br><br>max_degree = <span class="hljs-number">20</span>  <span class="hljs-comment"># 多项式的最大阶数</span><br>n_train, n_test = <span class="hljs-number">100</span>, <span class="hljs-number">100</span>  <span class="hljs-comment"># 训练和测试数据集大小</span><br>true_w = np.zeros(max_degree)  <span class="hljs-comment"># 分配大量的空间</span><br>true_w[<span class="hljs-number">0</span>:<span class="hljs-number">4</span>] = np.array([<span class="hljs-number">5</span>, <span class="hljs-number">1.2</span>, -<span class="hljs-number">3.4</span>, <span class="hljs-number">5.6</span>])  <span class="hljs-comment"># 设置真实权重，只有前四项是非零的</span><br><br><span class="hljs-comment"># 生成 (n_train + n_test) x 1 的特征矩阵，服从标准正态分布</span><br>features = np.random.normal(size=(n_train + n_test, <span class="hljs-number">1</span>))<br>np.random.shuffle(features)  <span class="hljs-comment"># 打乱特征顺序</span><br><br><span class="hljs-comment"># 生成多项式特征</span><br>poly_features = np.power(features, np.arange(max_degree).reshape(<span class="hljs-number">1</span>, -<span class="hljs-number">1</span>))<br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(max_degree):<br>    poly_features[:, i] /= math.gamma(i + <span class="hljs-number">1</span>)  <span class="hljs-comment"># gamma(n) = (n-1)!</span><br><br><span class="hljs-comment"># 生成标签</span><br>labels = np.dot(poly_features, true_w)<br>labels += np.random.normal(scale=<span class="hljs-number">0.1</span>, size=labels.shape)  <span class="hljs-comment"># 添加噪声</span><br><br><span class="hljs-comment"># NumPy ndarray转换为tensor</span><br>true_w, features, poly_features, labels = [torch.tensor(x, dtype=<br>	torch.float32) <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> [true_w, features, poly_features, labels]]<br><span class="hljs-built_in">print</span>(features[:<span class="hljs-number">2</span>], poly_features[:<span class="hljs-number">2</span>, :], labels[:<span class="hljs-number">2</span>])<br></code></pre></td></tr></table></figure>
<h5 id="对模型进行训练和测试"><a class="markdownIt-Anchor" href="#对模型进行训练和测试"></a> 对模型进行训练和测试</h5>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">train</span>(<span class="hljs-params">train_features, test_features, train_labels, test_labels,</span><br><span class="hljs-params">        num_epochs=<span class="hljs-number">400</span></span>):<br>    loss = nn.MSELoss(reduction=<span class="hljs-string">&#x27;none&#x27;</span>) <span class="hljs-comment"># 均方误差损失</span><br>    input_shape = train_features.shape[-<span class="hljs-number">1</span>] <br>    <span class="hljs-comment"># 不设置偏置，因为我们已经在多项式中实现了它</span><br>    net = nn.Sequential(nn.Linear(input_shape, <span class="hljs-number">1</span>, bias=<span class="hljs-literal">False</span>))<br>    batch_size = <span class="hljs-built_in">min</span>(<span class="hljs-number">10</span>, train_labels.shape[<span class="hljs-number">0</span>])<br>    train_iter = d2l.load_array((train_features, train_labels.reshape(-<span class="hljs-number">1</span>,<span class="hljs-number">1</span>)),<br>                 batch_size)<br>    test_iter = d2l.load_array((test_features, test_labels.reshape(-<span class="hljs-number">1</span>,<span class="hljs-number">1</span>)),<br>                 batch_size, is_train=<span class="hljs-literal">False</span>)<br>    trainer = torch.optim.SGD(net.parameters(), lr=<span class="hljs-number">0.01</span>)<br>    animator = d2l.Animator(xlabel=<span class="hljs-string">&#x27;epoch&#x27;</span>, ylabel=<span class="hljs-string">&#x27;loss&#x27;</span>, yscale=<span class="hljs-string">&#x27;log&#x27;</span>,<br>    xlim=[<span class="hljs-number">1</span>, num_epochs], ylim=[<span class="hljs-number">1e-3</span>, <span class="hljs-number">1e2</span>],<br>    legend=[<span class="hljs-string">&#x27;train&#x27;</span>, <span class="hljs-string">&#x27;test&#x27;</span>])<br>    <span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_epochs):<br>        d2l.train_epoch_ch3(net, train_iter, loss, trainer)<br>        <span class="hljs-keyword">if</span> epoch == <span class="hljs-number">0</span> <span class="hljs-keyword">or</span> (epoch + <span class="hljs-number">1</span>) % <span class="hljs-number">20</span> == <span class="hljs-number">0</span>:<br>            animator.add(epoch + <span class="hljs-number">1</span>, (d2l.evaluate_loss(net, train_iter, loss),<br>                                    d2l.evaluate_loss(net, test_iter, loss)))<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;weight:&#x27;</span>, net[<span class="hljs-number">0</span>].weight.data.numpy())<br></code></pre></td></tr></table></figure>
<h5 id="三阶多项式函数拟合正常"><a class="markdownIt-Anchor" href="#三阶多项式函数拟合正常"></a> 三阶多项式函数拟合（正常）</h5>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 从多项式特征中选择前4个维度，即1,x,x^2/2!,x^3/3!</span><br>train(poly_features[:n_train, :<span class="hljs-number">4</span>], poly_features[n_train:, :<span class="hljs-number">4</span>],<br>    labels[:n_train], labels[n_train:])<br></code></pre></td></tr></table></figure>
<p><img onerror="imgOnError(this);" data-fancybox="gallery" src="../images/posts/MLP/normfitting.png" alt="normfitting" data-caption="normfitting" loading="lazy"></p>
<h5 id="线性函数拟合欠拟合"><a class="markdownIt-Anchor" href="#线性函数拟合欠拟合"></a> 线性函数拟合（欠拟合）</h5>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 从多项式特征中选择前2个维度，即1和x</span><br>train(poly_features[:n_train, :<span class="hljs-number">2</span>], poly_features[n_train:, :<span class="hljs-number">2</span>],<br>labels[:n_train], labels[n_train:])<br></code></pre></td></tr></table></figure>
<p><img onerror="imgOnError(this);" data-fancybox="gallery" src="../images/posts/MLP/underfitting.png" alt="underfitting" data-caption="underfitting" loading="lazy"></p>
<ul>
<li>我们可以看到，这边虽然训练损失和测试损失离得很近，但是呢，二者都很高，所以就是一种欠拟合的体现。</li>
</ul>
<h5 id="高阶多项式函数拟合过拟合"><a class="markdownIt-Anchor" href="#高阶多项式函数拟合过拟合"></a> 高阶多项式函数拟合（过拟合）</h5>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 从多项式特征中选取所有维度</span><br>train(poly_features[:n_train, :], poly_features[n_train:, :],<br>    labels[:n_train], labels[n_train:], num_epochs=<span class="hljs-number">1500</span>)<br></code></pre></td></tr></table></figure>
<p><img onerror="imgOnError(this);" data-fancybox="gallery" src="../images/posts/MLP/overfitting.png" alt="overfitting" data-caption="overfitting" loading="lazy"></p>
<ul>
<li>（跑出了一个相当夸张，但是说服性很强的图）就是我们看这个图，它的训练损失很低，但是这个测试损失先降低后升高，结果还和训练损失距离拉大了。训练损失低是我们期望看到的，但是将这个和前面正常的情况进行比较，可以发现测试损失更高，这是我们所不期望看到的。这个图很好地体现了过拟合的现象。</li>
</ul>
<h2 id="权重衰减weight-decay"><a class="markdownIt-Anchor" href="#权重衰减weight-decay"></a> 权重衰减（weight decay）</h2>
<ul>
<li>
<p>可以使用<strong>正则化模型的技术</strong>来缓解过拟合。</p>
</li>
<li>
<p>在训练参数化机器学习模型时，权重衰减（weight decay）是最广泛使用的正则化的技术之一，它通常也被称为<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>L</mi><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">L_2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>​正则化.</p>
</li>
<li>
<p>最常用方法是<strong>将其范数作为惩罚项加到最小化损失的问题</strong>中</p>
<p class='katex-block'><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>L</mi><mo stretchy="false">(</mo><mi mathvariant="bold">w</mi><mo separator="true">,</mo><mi>b</mi><mo stretchy="false">)</mo><mo>+</mo><mfrac><mi>λ</mi><mn>2</mn></mfrac><mi mathvariant="normal">∣</mi><mi mathvariant="normal">∣</mi><mi mathvariant="bold">w</mi><mi mathvariant="normal">∣</mi><msup><mi mathvariant="normal">∣</mi><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">L(\mathbf w, b) + \frac{\lambda}{2}||\mathbf w||^2
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">L</span><span class="mopen">(</span><span class="mord mathbf" style="margin-right:0.01597em;">w</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal">b</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:2.05744em;vertical-align:-0.686em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.37144em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">2</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal">λ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mord">∣</span><span class="mord">∣</span><span class="mord mathbf" style="margin-right:0.01597em;">w</span><span class="mord">∣</span><span class="mord"><span class="mord">∣</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8641079999999999em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span></span></p>
<p>这里使用平方范数而不是标准范数（欧几里得范数（即平方范数开根））的原因是为了方便计算，是的倒数的和等于和的倒数。</p>
<p>使用<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>L</mi><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">L_2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>范数的一个原因是它<strong>对权重向量的大分量施加了巨大的惩罚</strong>。这使得我们的学习算法偏向于在大量特征上<strong>均匀分布权重</strong>的模型。在实践中，这可能使它们对单个变量中的观测误差更为稳定。相比之下，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>L</mi><mn>1</mn></msub></mrow><annotation encoding="application/x-tex">L_1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>惩罚会导致<strong>模型将权重集中在一小部分特征</strong>上，而将其他权重清除为零。这称为特征选择（feature selection），这可能是其他场景下需要的。</p>
</li>
<li>
<p>我们根据估计值与观测值之间的差异来更新w。然而，我们同时也在试图将w的大小缩小到零。这就是为什么这种方法有时被称为权重衰减</p>
</li>
</ul>
<h3 id="权重衰减从零实现"><a class="markdownIt-Anchor" href="#权重衰减从零实现"></a> 权重衰减从零实现</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn<br><span class="hljs-keyword">from</span> d2l <span class="hljs-keyword">import</span> torch <span class="hljs-keyword">as</span> d2l<br>n_train, n_test, num_inputs, batch_size = <span class="hljs-number">20</span>, <span class="hljs-number">100</span>, <span class="hljs-number">200</span>, <span class="hljs-number">5</span><br>true_w, true_b = torch.ones((num_inputs, <span class="hljs-number">1</span>)) * <span class="hljs-number">0.01</span>, <span class="hljs-number">0.05</span><br>train_data = d2l.synthetic_data(true_w, true_b, n_train)<br>train_iter = d2l.load_array(train_data, batch_size)<br>test_data = d2l.synthetic_data(true_w, true_b, n_test)<br>test_iter = d2l.load_array(test_data, batch_size, is_train=<span class="hljs-literal">False</span>)<br><br><span class="hljs-comment"># 初始化模型参数</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">init_params</span>():<br>    w = torch.normal(<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, size=(num_inputs, <span class="hljs-number">1</span>), requires_grad=<span class="hljs-literal">True</span>)<br>    b = torch.zeros(<span class="hljs-number">1</span>, requires_grad=<span class="hljs-literal">True</span>)<br>    <span class="hljs-keyword">return</span> [w, b]<br><span class="hljs-comment"># 定义L2范数惩罚</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">l2_penalty</span>(<span class="hljs-params">w</span>):<br>    <span class="hljs-keyword">return</span> torch.<span class="hljs-built_in">sum</span>(w.<span class="hljs-built_in">pow</span>(<span class="hljs-number">2</span>)) / <span class="hljs-number">2</span><br><span class="hljs-comment"># 训练</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">train</span>(<span class="hljs-params">lambd</span>):<br>    w, b = init_params()<br>    net, loss = <span class="hljs-keyword">lambda</span> X: d2l.linreg(X, w, b), d2l.squared_loss<br>    num_epochs, lr = <span class="hljs-number">100</span>, <span class="hljs-number">0.003</span><br>    animator = d2l.Animator(xlabel=<span class="hljs-string">&#x27;epochs&#x27;</span>, ylabel=<span class="hljs-string">&#x27;loss&#x27;</span>, yscale=<span class="hljs-string">&#x27;log&#x27;</span>,<br>                            xlim=[<span class="hljs-number">5</span>, num_epochs], legend=[<span class="hljs-string">&#x27;train&#x27;</span>, <span class="hljs-string">&#x27;test&#x27;</span>])<br>    <span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_epochs):<br>        <span class="hljs-keyword">for</span> X, y <span class="hljs-keyword">in</span> train_iter:<br>            <span class="hljs-comment"># 增加了L2范数惩罚项，</span><br>            <span class="hljs-comment"># 广播机制使l2_penalty(w)成为一个长度为batch_size的向量</span><br>            l = loss(net(X), y) + lambd * l2_penalty(w)<br>            l.<span class="hljs-built_in">sum</span>().backward()<br>            d2l.sgd([w, b], lr, batch_size)<br>        <span class="hljs-keyword">if</span> (epoch + <span class="hljs-number">1</span>) % <span class="hljs-number">5</span> == <span class="hljs-number">0</span>:<br>            animator.add(epoch + <span class="hljs-number">1</span>, (d2l.evaluate_loss(net, train_iter, loss),<br>                                    d2l.evaluate_loss(net, test_iter, loss)))<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;w的L2范数是：&#x27;</span>, torch.norm(w).item())<br><span class="hljs-comment"># 忽视正则化</span><br>train(lambd=<span class="hljs-number">0</span>)<br>d2l.plt.show()<br><span class="hljs-comment"># 使用权重衰减</span><br>train(lambd=<span class="hljs-number">3</span>)<br>d2l.plt.show()<br></code></pre></td></tr></table></figure>
<ul>
<li>忽视正则化</li>
</ul>
<p><img onerror="imgOnError(this);" data-fancybox="gallery" src="../images/posts/MLP/ignore.png" alt="ignore" data-caption="ignore" loading="lazy"></p>
<ul>
<li>使用权重衰减</li>
</ul>
<p><img onerror="imgOnError(this);" data-fancybox="gallery" src="../images/posts/MLP/reduction.png" alt="image-20240716191113986" data-caption="image-20240716191113986" loading="lazy"></p>
<ul>
<li>可以看到，忽视正则化的情况下过拟合现象极其明显（训练数据损失较低，但测试数据损失却很高）。使用了权重衰减以后，测试损失显著降低，缓和了过拟合的情况。</li>
</ul>
<h3 id="权重衰减的简洁实现"><a class="markdownIt-Anchor" href="#权重衰减的简洁实现"></a> 权重衰减的简洁实现</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn<br><span class="hljs-keyword">from</span> d2l <span class="hljs-keyword">import</span> torch <span class="hljs-keyword">as</span> d2l<br>n_train, n_test, num_inputs, batch_size = <span class="hljs-number">20</span>, <span class="hljs-number">100</span>, <span class="hljs-number">200</span>, <span class="hljs-number">5</span><br>true_w, true_b = torch.ones((num_inputs, <span class="hljs-number">1</span>)) * <span class="hljs-number">0.01</span>, <span class="hljs-number">0.05</span><br>train_data = d2l.synthetic_data(true_w, true_b, n_train)<br>train_iter = d2l.load_array(train_data, batch_size)<br>test_data = d2l.synthetic_data(true_w, true_b, n_test)<br>test_iter = d2l.load_array(test_data, batch_size, is_train=<span class="hljs-literal">False</span>)<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">train_concise</span>(<span class="hljs-params">wd</span>):<br>    net = nn.Sequential(nn.Linear(num_inputs, <span class="hljs-number">1</span>))<br>    <span class="hljs-keyword">for</span> param <span class="hljs-keyword">in</span> net.parameters():<br>        param.data.normal_()<br>        loss = nn.MSELoss(reduction=<span class="hljs-string">&#x27;none&#x27;</span>)<br>        num_epochs, lr = <span class="hljs-number">100</span>, <span class="hljs-number">0.003</span><br>        <span class="hljs-comment"># 偏置参数没有衰减 因为只给权重参数设置了&quot;weight_decay&quot;</span><br>        trainer = torch.optim.SGD([<br>        &#123;<span class="hljs-string">&quot;params&quot;</span>:net[<span class="hljs-number">0</span>].weight,<span class="hljs-string">&#x27;weight_decay&#x27;</span>: wd&#125;,<br>        &#123;<span class="hljs-string">&quot;params&quot;</span>:net[<span class="hljs-number">0</span>].bias&#125;], lr=lr)<br>        animator = d2l.Animator(xlabel=<span class="hljs-string">&#x27;epochs&#x27;</span>, ylabel=<span class="hljs-string">&#x27;loss&#x27;</span>, yscale=<span class="hljs-string">&#x27;log&#x27;</span>,<br>        xlim=[<span class="hljs-number">5</span>, num_epochs], legend=[<span class="hljs-string">&#x27;train&#x27;</span>, <span class="hljs-string">&#x27;test&#x27;</span>])<br>    <span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_epochs):<br>        <span class="hljs-keyword">for</span> X, y <span class="hljs-keyword">in</span> train_iter:<br>            trainer.zero_grad()<br>            l = loss(net(X), y)<br>            l.mean().backward()<br>            trainer.step()<br>        <span class="hljs-keyword">if</span> (epoch + <span class="hljs-number">1</span>) % <span class="hljs-number">5</span> == <span class="hljs-number">0</span>:<br>            animator.add(epoch + <span class="hljs-number">1</span>,<br>            (d2l.evaluate_loss(net, train_iter, loss),<br>            d2l.evaluate_loss(net, test_iter, loss)))<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;w的L2范数：&#x27;</span>, net[<span class="hljs-number">0</span>].weight.norm().item())<br>train_concise(<span class="hljs-number">0</span>)<br>train_concise(<span class="hljs-number">3</span>)<br></code></pre></td></tr></table></figure>
<h2 id="暂退法dropout"><a class="markdownIt-Anchor" href="#暂退法dropout"></a> 暂退法（Dropout）</h2>
<ul>
<li>
<p>泛化性和灵活性之间的这种基本权衡被描述为偏差*-*方差权衡（bias‐variance tradeoff）。<strong>线性模型</strong>有很高的偏差：它们只能表示一小类函数。然而，这些模型的方差很低：它们在不同的随机数据样本上可以得出相似的结果。<strong>深度神经网络</strong>位于偏差‐方差谱的另一端。与线性模型不同，神经网络并不局限于单独查看每个特征，而是学习特征之间的交互。但是，即使有比特征多得多的样本，深度神经网络也可能会过拟合。</p>
</li>
<li>
<p>深度网络具有泛化性。</p>
</li>
<li>
<p>在训练过程中，他们建议在<strong>计算后续层之前向网络的每一层注入噪声</strong>。因为当训练一个有多层的深层网络时，注入噪声只会在输入‐输出映射上增强平滑性（即函数不应该对其输入的微小变化敏感）。这个想法被称为<strong>暂退法（dropout）</strong>。（这种方法之所以被称为暂退法，因为我们从表面上看是在训练过程中丢弃（dropout）一些神经元。在整个训练过程的每一次迭代中，标准暂退法包括在计算下一层之前将当前层中的一些节点置零。）。</p>
</li>
<li>
<p>如何注入这种噪声？</p>
<ul>
<li>
<p>一种想法是以一种无偏向（unbiased）的方式注入噪声。</p>
</li>
<li>
<p>在标准暂退法正则化中，通过按保留（未丢弃）的节点的分数进行规范化来消除每一层的偏差，即每个中间活性值h以暂退概率p由随机变量h′替换。</p>
<p class='katex-block'><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msup><mi>h</mi><mo mathvariant="normal">′</mo></msup><mo>=</mo><mrow><mo fence="true">{</mo><mtable rowspacing="0.24999999999999992em" columnalign="right left right left" columnspacing="0em 1em 0em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mn>0</mn></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mtext>概率为</mtext><mi>p</mi></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mfrac><mi>h</mi><mrow><mn>1</mn><mo>−</mo><mi>p</mi></mrow></mfrac></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mtext>其他情况</mtext></mrow></mstyle></mtd></mtr></mtable></mrow></mrow><annotation encoding="application/x-tex">h^\prime = \left\{
	\begin{aligned}
	&amp;0 &amp; &amp;概率为p \\
	&amp;\frac{h}{1-p} &amp; &amp;其他情况
	\end{aligned}
\right .
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.801892em;vertical-align:0em;"></span><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.801892em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:4.2000399999999996em;vertical-align:-1.8500199999999998em;"></span><span class="minner"><span class="mopen"><span class="delimsizing mult"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:2.35002em;"><span style="top:-2.19999em;"><span class="pstrut" style="height:3.15em;"></span><span class="delimsizinginner delim-size4"><span>⎩</span></span></span><span style="top:-2.19499em;"><span class="pstrut" style="height:3.15em;"></span><span class="delimsizinginner delim-size4"><span>⎪</span></span></span><span style="top:-2.20499em;"><span class="pstrut" style="height:3.15em;"></span><span class="delimsizinginner delim-size4"><span>⎪</span></span></span><span style="top:-3.15001em;"><span class="pstrut" style="height:3.15em;"></span><span class="delimsizinginner delim-size4"><span>⎨</span></span></span><span style="top:-4.2950099999999996em;"><span class="pstrut" style="height:3.15em;"></span><span class="delimsizinginner delim-size4"><span>⎪</span></span></span><span style="top:-4.30501em;"><span class="pstrut" style="height:3.15em;"></span><span class="delimsizinginner delim-size4"><span>⎪</span></span></span><span style="top:-4.60002em;"><span class="pstrut" style="height:3.15em;"></span><span class="delimsizinginner delim-size4"><span>⎧</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.8500199999999998em;"><span></span></span></span></span></span></span><span class="mord"><span class="mtable"><span class="col-align-r"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:2.2759400000000003em;"><span style="top:-4.80738em;"><span class="pstrut" style="height:3.3714399999999998em;"></span><span class="mord"></span></span><span style="top:-2.7759399999999994em;"><span class="pstrut" style="height:3.3714399999999998em;"></span><span class="mord"></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.7759400000000003em;"><span></span></span></span></span></span><span class="col-align-l"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:2.2759400000000003em;"><span style="top:-4.80738em;"><span class="pstrut" style="height:3.3714399999999998em;"></span><span class="mord"><span class="mord"></span><span class="mord">0</span></span></span><span style="top:-2.7759399999999994em;"><span class="pstrut" style="height:3.3714399999999998em;"></span><span class="mord"><span class="mord"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.37144em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">1</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord mathnormal">p</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal">h</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.8804400000000001em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.7759400000000003em;"><span></span></span></span></span></span><span class="arraycolsep" style="width:1em;"></span><span class="col-align-r"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:2.2759400000000003em;"><span style="top:-4.80738em;"><span class="pstrut" style="height:3.3714399999999998em;"></span><span class="mord"></span></span><span style="top:-2.7759399999999994em;"><span class="pstrut" style="height:3.3714399999999998em;"></span><span class="mord"></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.7759400000000003em;"><span></span></span></span></span></span><span class="col-align-l"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:2.2759400000000003em;"><span style="top:-4.80738em;"><span class="pstrut" style="height:3.3714399999999998em;"></span><span class="mord"><span class="mord"></span><span class="mord cjk_fallback">概</span><span class="mord cjk_fallback">率</span><span class="mord cjk_fallback">为</span><span class="mord mathnormal">p</span></span></span><span style="top:-2.7759399999999994em;"><span class="pstrut" style="height:3.3714399999999998em;"></span><span class="mord"><span class="mord"></span><span class="mord cjk_fallback">其</span><span class="mord cjk_fallback">他</span><span class="mord cjk_fallback">情</span><span class="mord cjk_fallback">况</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.7759400000000003em;"><span></span></span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span></p>
<p>期望仍保持不变。</p>
<ul>
<li>关于这边的中间活性值！
<ul>
<li>假设我们有一个简单的神经网络，其某一层的输出表示为 h。这个 h 就是该层的中间活性值，它将被作为下一层的输入。在应用 Dropout 时，这些中间活性值 h 会以某个概率 p 被暂时丢弃，即被置为零。</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>
<p>总之呢，应用于隐藏层，就是按照概率随机丢掉几个隐藏单元。</p>
</li>
</ul>
<h3 id="从零开始实现"><a class="markdownIt-Anchor" href="#从零开始实现"></a> 从零开始实现</h3>
<ul>
<li>
<p>dropout_layer函数</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn<br><span class="hljs-keyword">from</span> d2l <span class="hljs-keyword">import</span> torch <span class="hljs-keyword">as</span> d2l<br><span class="hljs-keyword">def</span> <span class="hljs-title function_">dropout_layer</span>(<span class="hljs-params">X, dropout</span>):<br>    <span class="hljs-keyword">assert</span> <span class="hljs-number">0</span> &lt;= dropout &lt;= <span class="hljs-number">1</span><br>    <span class="hljs-comment"># 在本情况中，所有元素都被丢弃</span><br>    <span class="hljs-keyword">if</span> dropout == <span class="hljs-number">1</span>:<br>        <span class="hljs-keyword">return</span> torch.zeros_like(X)<br>    <span class="hljs-comment"># 在本情况中，所有元素都被保留</span><br>    <span class="hljs-keyword">if</span> dropout == <span class="hljs-number">0</span>:<br>        <span class="hljs-keyword">return</span> X<br>    mask = (torch.rand(X.shape) &gt; dropout).<span class="hljs-built_in">float</span>()<br>    <span class="hljs-keyword">return</span> mask * X / (<span class="hljs-number">1.0</span> - dropout)<br><br>X= torch.arange(<span class="hljs-number">16</span>, dtype = torch.float32).reshape((<span class="hljs-number">2</span>, <span class="hljs-number">8</span>))<br><span class="hljs-built_in">print</span>(X)<br><span class="hljs-built_in">print</span>(dropout_layer(X, <span class="hljs-number">0.</span>))<br><span class="hljs-built_in">print</span>(dropout_layer(X, <span class="hljs-number">0.5</span>))<br><span class="hljs-built_in">print</span>(dropout_layer(X, <span class="hljs-number">1.</span>))<br></code></pre></td></tr></table></figure>
</li>
<li>
<p>定义模型参数：定义具有两个隐藏层的多层感知机</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">num_inputs, num_outputs, num_hiddens1, num_hiddens2 = <span class="hljs-number">784</span>, <span class="hljs-number">10</span>, <span class="hljs-number">256</span>, <span class="hljs-number">256</span><br></code></pre></td></tr></table></figure>
</li>
<li>
<p>定义模型:可以将暂退法应用于每个隐藏层的输出（在激活函数之后），并且可以为每一层分别设置暂退概率：常见的技巧是在靠近输入层的地方设置较低的暂退概率。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 模型</span><br>dropout1, dropout2 = <span class="hljs-number">0.2</span>, <span class="hljs-number">0.5</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Net</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, num_inputs, num_outputs, num_hiddens1, num_hiddens2,</span><br><span class="hljs-params">        is_training = <span class="hljs-literal">True</span></span>):<br>        <span class="hljs-built_in">super</span>(Net, <span class="hljs-variable language_">self</span>).__init__()<br>        <span class="hljs-variable language_">self</span>.num_inputs = num_inputs<br>        <span class="hljs-variable language_">self</span>.training = is_training<br>        <span class="hljs-variable language_">self</span>.lin1 = nn.Linear(num_inputs, num_hiddens1)<br>        <span class="hljs-variable language_">self</span>.lin2 = nn.Linear(num_hiddens1, num_hiddens2)<br>        <span class="hljs-variable language_">self</span>.lin3 = nn.Linear(num_hiddens2, num_outputs)<br>        <span class="hljs-variable language_">self</span>.relu = nn.ReLU()<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, X</span>):<br>        H1 = <span class="hljs-variable language_">self</span>.relu(<span class="hljs-variable language_">self</span>.lin1(X.reshape((-<span class="hljs-number">1</span>, <span class="hljs-variable language_">self</span>.num_inputs))))<br>        <span class="hljs-comment"># 只有在训练模型时才使用dropout</span><br>        <span class="hljs-keyword">if</span> <span class="hljs-variable language_">self</span>.training == <span class="hljs-literal">True</span>:<br>            <span class="hljs-comment"># 在第一个全连接层之后添加一个dropout层</span><br>            H1 = dropout_layer(H1, dropout1)<br>        H2 = <span class="hljs-variable language_">self</span>.relu(<span class="hljs-variable language_">self</span>.lin2(H1))<br>        <span class="hljs-keyword">if</span> <span class="hljs-variable language_">self</span>.training == <span class="hljs-literal">True</span>:<br>            <span class="hljs-comment"># 在第二个全连接层之后添加一个dropout层</span><br>            H2 = dropout_layer(H2, dropout2)<br>        out = <span class="hljs-variable language_">self</span>.lin3(H2)<br>        <span class="hljs-keyword">return</span> out<br>net = Net(num_inputs, num_outputs, num_hiddens1, num_hiddens2)<br></code></pre></td></tr></table></figure>
<p>我们在<code>forward</code>函数中可以看到，<code>dropout_layer</code>是在<code>relu</code>激活函数以后的！也就是在激活函数以后在进行dropout操作！</p>
</li>
<li>
<p>训练和测试</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">num_epochs, lr, batch_size = <span class="hljs-number">10</span>, <span class="hljs-number">0.5</span>, <span class="hljs-number">256</span><br>loss = nn.CrossEntropyLoss(reduction=<span class="hljs-string">&#x27;none&#x27;</span>)<br>train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)<br>trainer = torch.optim.SGD(net.parameters(), lr=lr)<br>d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)<br></code></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="简洁实现"><a class="markdownIt-Anchor" href="#简洁实现"></a> 简洁实现</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn<br><span class="hljs-keyword">from</span> d2l <span class="hljs-keyword">import</span> torch <span class="hljs-keyword">as</span> d2l<br>num_inputs, num_outputs, num_hiddens1, num_hiddens2 = <span class="hljs-number">784</span>, <span class="hljs-number">10</span>, <span class="hljs-number">256</span>, <span class="hljs-number">256</span><br>dropout1, dropout2 = <span class="hljs-number">0.2</span>, <span class="hljs-number">0.5</span><br>net = nn.Sequential(nn.Flatten(),<br>                    nn.Linear(<span class="hljs-number">784</span>, <span class="hljs-number">256</span>),<br>                    nn.ReLU(),<br>                    <span class="hljs-comment"># 在第一个全连接层之后添加一个dropout层</span><br>                    nn.Dropout(dropout1),<br>                    nn.Linear(<span class="hljs-number">256</span>, <span class="hljs-number">256</span>),<br>                    nn.ReLU(),<br>                    <span class="hljs-comment"># 在第二个全连接层之后添加一个dropout层</span><br>                    nn.Dropout(dropout2),<br>                    nn.Linear(<span class="hljs-number">256</span>, <span class="hljs-number">10</span>))<br><span class="hljs-keyword">def</span> <span class="hljs-title function_">init_weights</span>(<span class="hljs-params">m</span>):<br>    <span class="hljs-keyword">if</span> <span class="hljs-built_in">type</span>(m) == nn.Linear:<br>        nn.init.normal_(m.weight, std=<span class="hljs-number">0.01</span>)<br>net.apply(init_weights)<br><br>num_epochs, lr, batch_size = <span class="hljs-number">10</span>, <span class="hljs-number">0.5</span>, <span class="hljs-number">256</span><br>loss = nn.CrossEntropyLoss(reduction=<span class="hljs-string">&#x27;none&#x27;</span>)<br>train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)<br>trainer = torch.optim.SGD(net.parameters(), lr=lr)<br>d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)<br></code></pre></td></tr></table></figure>
<h2 id="前向传播-反向传播和计算图"><a class="markdownIt-Anchor" href="#前向传播-反向传播和计算图"></a> 前向传播、反向传播和计算图</h2>
<ul>
<li><strong>前向传播（forward propagation或forward pass）<strong>指的是：按</strong>顺序</strong>（从输入层到输出层）计算和存储神经网络中<strong>每层的结果</strong>。</li>
<li><strong>反向传播（backward propagation或backpropagation）<strong>指的是</strong>计算神经网络参数梯度</strong>的方法。简言之，该方法根据微积分中的链式规则，按相反的顺序从输出层到输入层遍历网络。</li>
</ul>
<h2 id="数值稳定性和模型初始化"><a class="markdownIt-Anchor" href="#数值稳定性和模型初始化"></a> 数值稳定性和模型初始化</h2>
<ul>
<li>
<p>初始化方案的选择在神经网络学习中起着举足轻重的作用，它对<strong>保持数值稳定性</strong>至关重要。此外，这些<strong>初始化方案的选择可以与非线性激活函数的选择</strong>有趣的结合在一起。我们选择哪个函数以及如何初始化参数可以决定优化算法收敛的速度有多快。糟糕选择可能会导致我们在训练时遇到梯度爆炸或梯度消失。</p>
</li>
<li>
<p>不稳定梯度带来的风险不止在于数值表示；不稳定梯度也威胁到我们优化算法的稳定性。我们可能面临一些问题。要么是**梯度爆炸（gradient exploding）<strong>问题：<strong>参数更新过大</strong>，破坏了模型的稳定收敛；要么是</strong>梯度消失（gradient vanishing）**问题：<strong>参数更新过小</strong>，在每次更新时几乎不会移动，导致模型无法学习。</p>
</li>
<li>
<p>梯度消失：以sigmoid函数为例：</p>
<ul>
<li>当sigmoid函数的输入很大或是很小时，它的梯度都会消失。</li>
<li>此外，当反向传播通过许多层时，除非我们在刚刚好的地方，这些地方sigmoid函数的输入接近于零，否则整个乘积的梯度可能会消失。</li>
</ul>
</li>
</ul>
<h3 id="参数初始化"><a class="markdownIt-Anchor" href="#参数初始化"></a> 参数初始化</h3>
<ul>
<li>
<p>参数初始化可以帮助解决或者减轻上面的问题。</p>
</li>
<li>
<p><strong>默认初始化</strong>：前面我们都使用正态分布来初始化权重值。如果我们不指定初始化方法，框架将使用默认的随机初始化方法，对于中等难度的问题，这种方法通常很有效。</p>
</li>
<li>
<p><strong>Xavier</strong>初始化：（粗略的看了一眼，但其实没看懂！）</p>
<ul>
<li>通常从均值为零，方差<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>σ</mi><mn>2</mn></msup><mo>=</mo><mfrac><mn>2</mn><mrow><msub><mi>n</mi><mrow><mi>i</mi><mi>n</mi></mrow></msub><mo>+</mo><msub><mi>n</mi><mrow><mi>o</mi><mi>u</mi><mi>t</mi></mrow></msub></mrow></mfrac></mrow><annotation encoding="application/x-tex">\sigma^2 = \frac{2}{n_{in} + n_{out}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141079999999999em;vertical-align:0em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">σ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.2902079999999998em;vertical-align:-0.44509999999999994em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.845108em;"><span style="top:-2.655em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight">n</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3280857142857143em;"><span style="top:-2.357em;margin-left:0em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mord mathnormal mtight">n</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span><span class="mbin mtight">+</span><span class="mord mtight"><span class="mord mathnormal mtight">n</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.29634285714285713em;"><span style="top:-2.357em;margin-left:0em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight">u</span><span class="mord mathnormal mtight">t</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.44509999999999994em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span>的高斯分布中采样权重。</li>
</ul>
</li>
</ul>
<h2 id="环境和分布偏移"><a class="markdownIt-Anchor" href="#环境和分布偏移"></a> 环境和分布偏移</h2>
<ul>
<li>协变量偏移：虽然输入的分布可能随时间而改变，但标签函数（即条件分布<em>P</em>(<em>y</em> <em>|</em> <strong>x</strong>)）没有改变。统计学家称之为协变量偏移（covariate shift），因为这个问题是由于协变量（特征）分布的变化而产生的。</li>
<li>标签偏移：假设标签边缘概率<em>P</em>(<em>y</em>)可以改变，但是类别条件分布<em>P</em>(<strong>x</strong> <em>|</em> <em>y</em>)在不同的领域之间保持不变。</li>
<li>概念偏移：（字面意思！）</li>
<li>下面还有各种偏移的纠偏！！不想看喵！！之后需要的话再滚回来看！！</li>
</ul>
<blockquote>
<p>最后，重要的是，当我们部署机器学习系统时，不仅仅是在优化一个预测模型，而通常是在提供一个会被用来（部分或完全）进行自动化决策的工具。这些技术系统可能会通过其进行的决定而影响到每个人的生活。</p>
</blockquote>
<p>然后是Kaggle实战！这里就省略啦！</p>
<h2 id="ff的碎碎念"><a class="markdownIt-Anchor" href="#ff的碎碎念"></a> ff的碎碎念！</h2>
<p>终于摸完了这一章了qaq但是还有一些部分匆忙地就过去了，希望能有点印象叭。</p>
<p>这两天突然听说好多大佬都去找了夏令营 突然就觉得好沮丧 而我连基础知识都不会 好难过<br />
而且也删除喜欢的男孩子的好友惹，还是会有点小难过，但是确实要成长一下了，至少也要有自己处理现在的逆境的能力呀，才可以不给喜欢的男孩子带来负担呢qaq希望他也能一切顺利qaq<br />
真的刚刚突然焦虑值upupup，在想自己为什么学这么慢，为什么什么都不会，到现在论文也没有看几篇，好多东西都不会却还玩物丧志，真的难过到爆炸了。感觉这段时间真的会很难熬呢QAQ。<br />
但是，其实静下心来想想，ff做好现在的事情就好惹，一步一步捡起自己没学到的知识，一点一点去探索需要探索的领域。加油油喵。</p>
<p>呜呜呜呜路漫漫其修远兮，吾将上下而求索。呜呜呜呜。</p>

    <p>今天大概也在努力吧~o(*￣▽￣*)ブ</p>

  </article>

  
      
    <div class="nexmoe-post-copyright">
        <strong>本文作者：</strong>YXF<br>
        <strong>本文链接：</strong><a href="https://yxf203.github.io/2024/07/17/%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA/" title="https:&#x2F;&#x2F;yxf203.github.io&#x2F;2024&#x2F;07&#x2F;17&#x2F;%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA&#x2F;" target="_blank" rel="noopener">https:&#x2F;&#x2F;yxf203.github.io&#x2F;2024&#x2F;07&#x2F;17&#x2F;%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA&#x2F;</a><br>
        
            <strong>版权声明：</strong>本文采用 <a href="https://creativecommons.org/licenses/by-nc-sa/3.0/cn/deed.zh" target="_blank">CC BY-NC-SA 3.0 CN</a> 协议进行许可

        
    </div>


  
  
  <div class="nexmoe-post-meta nexmoe-rainbow">
   
    
        <a class="nexmoefont icon-tag-fill -none-link" href="/tags/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" rel="tag">动手学深度学习</a>
    
</div>
  
  
    <script async src="/js/copy-codeblock.js?v=1721214559364"></script>
  

  
      <div class="nexmoe-post-footer">
          
      </div>
  
</div></div><div class="nexmoe-post-right">    <div class="nexmoe-fixed">
        <div class="nexmoe-tool">

            

            
            
            <button class="mdui-fab catalog" style="overflow:unset;">
                <i class="nexmoefont icon-i-catalog"></i>
                <div class="nexmoe-toc">
                    <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA"><span class="toc-number">1.</span> <span class="toc-text"> 多层感知机</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%9A%90%E8%97%8F%E5%B1%82"><span class="toc-number">1.1.</span> <span class="toc-text"> 隐藏层</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0"><span class="toc-number">1.2.</span> <span class="toc-text"> 激活函数</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#relu%E5%87%BD%E6%95%B0rectified-linear-unit"><span class="toc-number">1.2.1.</span> <span class="toc-text"> ReLU函数（Rectified linear unit）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#sigmoid%E5%87%BD%E6%95%B0"><span class="toc-number">1.2.2.</span> <span class="toc-text"> sigmoid函数</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#tanh%E5%87%BD%E6%95%B0"><span class="toc-number">1.2.3.</span> <span class="toc-text"> tanh函数</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA%E7%9A%84%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E5%AE%9E%E7%8E%B0"><span class="toc-number">2.</span> <span class="toc-text"> 多层感知机的从零开始实现</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AF%BB%E5%8F%96%E6%95%B0%E6%8D%AE%E5%92%8C%E5%88%9D%E5%A7%8B%E5%8C%96%E6%A8%A1%E5%9E%8B%E5%8F%82%E6%95%B0"><span class="toc-number">2.1.</span> <span class="toc-text"> 读取数据和初始化模型参数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0-2"><span class="toc-number">2.2.</span> <span class="toc-text"> 激活函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B"><span class="toc-number">2.3.</span> <span class="toc-text"> 模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E4%B8%8E%E8%AE%AD%E7%BB%83"><span class="toc-number">2.4.</span> <span class="toc-text"> 损失函数与训练</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA%E7%9A%84%E7%AE%80%E6%B4%81%E5%AE%9E%E7%8E%B0"><span class="toc-number">3.</span> <span class="toc-text"> 多层感知机的简洁实现</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E9%80%89%E6%8B%A9-%E6%AC%A0%E6%8B%9F%E5%90%88%E5%92%8C%E8%BF%87%E6%8B%9F%E5%90%88"><span class="toc-number">4.</span> <span class="toc-text"> 模型选择、欠拟合和过拟合</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83%E8%AF%AF%E5%B7%AE%E5%92%8C%E6%B3%9B%E5%8C%96%E8%AF%AF%E5%B7%AE"><span class="toc-number">4.1.</span> <span class="toc-text"> 训练误差和泛化误差</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E9%80%89%E6%8B%A9"><span class="toc-number">4.2.</span> <span class="toc-text"> 模型选择</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%AA%8C%E8%AF%81%E9%9B%86"><span class="toc-number">4.2.1.</span> <span class="toc-text"> 验证集</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%AC%A0%E6%8B%9F%E5%90%88%E8%BF%98%E6%98%AF%E8%BF%87%E6%8B%9F%E5%90%88"><span class="toc-number">4.2.2.</span> <span class="toc-text"> 欠拟合还是过拟合？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%A4%9A%E9%A1%B9%E5%BC%8F%E5%9B%9E%E5%BD%92"><span class="toc-number">4.2.3.</span> <span class="toc-text"> 多项式回归</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E7%94%9F%E6%88%90%E6%95%B0%E6%8D%AE"><span class="toc-number">4.2.3.1.</span> <span class="toc-text"> 生成数据</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%AF%B9%E6%A8%A1%E5%9E%8B%E8%BF%9B%E8%A1%8C%E8%AE%AD%E7%BB%83%E5%92%8C%E6%B5%8B%E8%AF%95"><span class="toc-number">4.2.3.2.</span> <span class="toc-text"> 对模型进行训练和测试</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E4%B8%89%E9%98%B6%E5%A4%9A%E9%A1%B9%E5%BC%8F%E5%87%BD%E6%95%B0%E6%8B%9F%E5%90%88%E6%AD%A3%E5%B8%B8"><span class="toc-number">4.2.3.3.</span> <span class="toc-text"> 三阶多项式函数拟合（正常）</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E7%BA%BF%E6%80%A7%E5%87%BD%E6%95%B0%E6%8B%9F%E5%90%88%E6%AC%A0%E6%8B%9F%E5%90%88"><span class="toc-number">4.2.3.4.</span> <span class="toc-text"> 线性函数拟合（欠拟合）</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E9%AB%98%E9%98%B6%E5%A4%9A%E9%A1%B9%E5%BC%8F%E5%87%BD%E6%95%B0%E6%8B%9F%E5%90%88%E8%BF%87%E6%8B%9F%E5%90%88"><span class="toc-number">4.2.3.5.</span> <span class="toc-text"> 高阶多项式函数拟合（过拟合）</span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%9D%83%E9%87%8D%E8%A1%B0%E5%87%8Fweight-decay"><span class="toc-number">5.</span> <span class="toc-text"> 权重衰减（weight decay）</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9D%83%E9%87%8D%E8%A1%B0%E5%87%8F%E4%BB%8E%E9%9B%B6%E5%AE%9E%E7%8E%B0"><span class="toc-number">5.1.</span> <span class="toc-text"> 权重衰减从零实现</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9D%83%E9%87%8D%E8%A1%B0%E5%87%8F%E7%9A%84%E7%AE%80%E6%B4%81%E5%AE%9E%E7%8E%B0"><span class="toc-number">5.2.</span> <span class="toc-text"> 权重衰减的简洁实现</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%9A%82%E9%80%80%E6%B3%95dropout"><span class="toc-number">6.</span> <span class="toc-text"> 暂退法（Dropout）</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E5%AE%9E%E7%8E%B0"><span class="toc-number">6.1.</span> <span class="toc-text"> 从零开始实现</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AE%80%E6%B4%81%E5%AE%9E%E7%8E%B0"><span class="toc-number">6.2.</span> <span class="toc-text"> 简洁实现</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%89%8D%E5%90%91%E4%BC%A0%E6%92%AD-%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E5%92%8C%E8%AE%A1%E7%AE%97%E5%9B%BE"><span class="toc-number">7.</span> <span class="toc-text"> 前向传播、反向传播和计算图</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%95%B0%E5%80%BC%E7%A8%B3%E5%AE%9A%E6%80%A7%E5%92%8C%E6%A8%A1%E5%9E%8B%E5%88%9D%E5%A7%8B%E5%8C%96"><span class="toc-number">8.</span> <span class="toc-text"> 数值稳定性和模型初始化</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8F%82%E6%95%B0%E5%88%9D%E5%A7%8B%E5%8C%96"><span class="toc-number">8.1.</span> <span class="toc-text"> 参数初始化</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%8E%AF%E5%A2%83%E5%92%8C%E5%88%86%E5%B8%83%E5%81%8F%E7%A7%BB"><span class="toc-number">9.</span> <span class="toc-text"> 环境和分布偏移</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#ff%E7%9A%84%E7%A2%8E%E7%A2%8E%E5%BF%B5"><span class="toc-number">10.</span> <span class="toc-text"> ff的碎碎念！</span></a></li></ol>
                </div>
            </button>
            

            

            <a href="#nexmoe-content" class="backtop toc-link" aria-label="Back To Top" title="top"><button class="mdui-fab mdui-ripple"><i class="nexmoefont icon-caret-top"></i></button></a>
        </div>
    </div>
</div></div><div id="nexmoe-footer"><!--!--></div><div id="nexmoe-search-space"><div class="search-container"><div class="search-header"><div class="search-input-container"><input class="search-input" type="text" placeholder="搜索" onInput="sinput();"></div><a class="search-close" onclick="sclose();">×</a></div><div class="search-body"></div></div></div><div><script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-2058306854838448" crossorigin="anonymous"></script>
</div></body></html>