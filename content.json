{"meta":{"title":"fff从零开始的博客OvO","subtitle":"从零开始的……","description":"","author":"YXF","url":"https://yxf203.github.io","root":"/"},"pages":[{"title":"文章归档","date":"2024-03-07T11:07:58.070Z","updated":"2024-03-07T11:07:58.070Z","comments":true,"path":"archive.html","permalink":"https://yxf203.github.io/archive.html","excerpt":"","text":""},{"title":"","date":"2024-05-19T09:42:06.724Z","updated":"2024-05-19T09:42:06.724Z","comments":true,"path":"custom.css","permalink":"https://yxf203.github.io/custom.css","excerpt":"","text":":root { --gutter: 25px; --radius: 13px; --color-primary: #000; --color2: #ff761e; --color3: #ffb900; --color4: #33d57a; --color5: #00dbff; --color6: #1a98ff; --color7: #9090ff; --color-primary-bg: #d6ecf0; --color2-bg: rgba(255,118,30,0.15); --color3-bg: rgba(255,185,0,0.15); --color4-bg: rgba(51,213,122,0.15); --color5-bg: rgba(0,219,255,0.15); --color6-bg: rgba(26,152,255,0.15); --color7-bg: rgba(144,144,255,0.15); --color-shadow: rgba(161, 177, 204, 0.4); } .nexmoe-rainbow a { background-color: #d6ecf0; color: #000; box-shadow: 1px 1px 1px 1px rgba(0, 0, 0, 0.5); } .katex-html { display: none; } article ul span { margin-right: 0; }"}],"posts":[{"title":"线性神经网络","slug":"线性神经网络","date":"2024-07-15T10:00:45.000Z","updated":"2024-07-15T11:28:38.830Z","comments":true,"path":"2024/07/15/线性神经网络/","link":"","permalink":"https://yxf203.github.io/2024/07/15/%E7%BA%BF%E6%80%A7%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/","excerpt":"","text":"经典统计学习技术中的线性回归和 softmax 回归可以视为线性神经网络。 线性回归 回归（regression）是能为一个或多个自变量与因变量之间关系建模的一类方法。 正如在引言里提及的那样，回归问题往往涉及到预测一个数值。 基本元素 假设 假设自变量x和因变量yyy之间的关系是线性的，即y可以表示为x中元素的加权和，这里通常允许包含观测值的一些噪声。 假设任何噪声都比较正常，如噪声遵循正态分布。 线性模型 举个实际的例子：我们希望根据房屋的面积（平方英尺）和房龄（年）来估算房屋价格（美元）。 为了能预测，需要获取数据集（包括房屋的销售价格、面积和房龄）。 该数据集称为训练数据集（training data set）或训练集（training set）。每行数据（比如一次房屋交易相对应的数据）称为样本（sample），也可以称为数据点（data point）或数据样本（data instance）。我们把试图预测的目标（比如预测房屋价格）称为标签（label）或目标（target）。预测所依据的自变量（面积和房龄）称为特征（feature）或协变量（covariate）。 对于上面的举例，我们可以做出线性假设（目标（房屋价格）可以表示为特征（面积和房龄）的加权和） price=warea⋅area+wage⋅age+bprice = w_{area}·area+w_{age}·age+b price=warea​⋅area+wage​⋅age+b wareaw_{area}warea​和wagew_{age}wage​称为权重。决定每个特征值对我们预测值的影响。 bbb称为偏置（bias）、偏移量（offset）或截距（intercept）。偏置是指当所有特征都取值为 0 时，预测值应该为多少。（如果没有的话，我们模型的表达能力将受到限制。） 上面的式子是输入特征的一个仿射变换（affine transformation）。仿射变换的特点是通过加权和对特征进行线性变换（linear transformation），并通过偏置项来进行平移（translation）。 而给定一个数据集，目标——寻找模型的权重w和偏置 b。 对于单个特征向量x： y^=wTx+b\\hat{y} = \\mathbf{w}^T\\mathbf{x} + b y^​=wTx+b 对于特征集合： y^=Xw+b\\mathbf{\\hat y} = \\mathbf{Xw} + b y^​=Xw+b 损失函数 损失函数（loss function） 能够量化目标的实际值与预测值之间的差距。通常选择非负数作为损失，且数值越小表示损失越小。 回归问题中最常用的损失函数是平方误差函数，即： l(i)(w,b)=12(y^(i)−y(i))2l^{(i)}(\\mathbf{w}, b) = \\frac{1}{2}\\left(\\hat{y}^{(i)}-y^{(i)} \\right)^2 l(i)(w,b)=21​(y^​(i)−y(i))2 度量模型在整个数据集上的质量，我们需要计算在训练集 n 个样本上的损失均值（也等价于求和）。 L(w,b)=1n∑i=1nl(i)(w,b)=1n∑i=1n12(wTx(i)+b−y(i))2L(\\mathbf{w}, b) = \\frac{1}{n}\\sum_{i=1}^{n}l^{(i)}(\\mathbf{w},b)=\\frac{1}{n}\\sum_{i=1}^{n}\\frac{1}{2}\\left(\\mathbf{w}^T\\mathbf{x}^{(i)}+b-y^{(i)} \\right)^2 L(w,b)=n1​i=1∑n​l(i)(w,b)=n1​i=1∑n​21​(wTx(i)+b−y(i))2 我们只需求出使$L(\\mathbf{w}, b) $最小的w*,b*即可。 解析解 线性回归的解可以用一个公式简单地表达出来，这类解叫作解析解（analytical solution） 随机梯度下降 通过不断地在损失函数递减的方向上更新参数来降低误差。 小批量随机梯度下降（minibatch stochastic gradient descent）：通常会在每次需要计算更新的时候随机抽取一小批样本，从而加快执行速度。 在每次迭代中，我们首先随机抽样一个小批量 B，它是由固定数量的训练样本组成的。然后，我们计算小批量的平均损失关于模型参数的导数（也可以称为梯度）。最后，我们将梯度乘以一个预先确定的正数 η，并从当前参数的值中减掉。 算法步骤如下： （1）初始化模型参数的值，如随机初始化； （2）从数据集中随机抽取小批量样本且在负梯度的方向上更新参数，并不断迭代这一步骤。 |B|表示每个小批量中的样本数，这也称为批量大小（batch size）。η 表示学习率（learning rate）。这两个都需要手动预先指定。这些可以调整但不在训练过程中更新的参数称为超参数（hyperparameter）。调参（hyperparameter tuning）是选择超参数的过程。超参数通常是我们根据训练迭代结果来调整的，而训练迭代结果是在独立的验证数据集（validation dataset）上评估得到的。 事实上，更难做到的是找到一组参数，这组参数能够在我们从未见过的数据上实现较低的损失，这一挑战被称为泛化（generalization）。 矢量化加速 这部分原书利用 d2l 包中的 Timer 类，比较了使用 for 循环完成两个向量相加和直接依赖‘+’实现两个向量相加所用的时间。说明矢量化的速度要来得快的多。 正态分布与平方损失 正态分布（normal distribution），也称为高斯分布（Gaussian distribution）。若随机变量x具有均值µ和方差σ2（标准差σ），其正态分布概率密度函数如下： p(x)=12πσ2exp(−(x−μ)22σ2)p(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}exp(-\\frac{(x-\\mu)^2}{2\\sigma^2}) p(x)=2πσ2​1​exp(−2σ2(x−μ)2​) （极大似然忘掉啦！如果有需要的话再补补看吧！） 神经网络图 对于线性回归，每个输入都与每个输出（在本例中只有一个输出）相连，我们将这种变换（图 3.1.2 中的输出层）称为全连接层（fully‐connected layer）或称为稠密层（dense layer）。 线性回归的从零开始实现 生成数据集 12345678import randomimport torchfrom d2l import torch as d2ltrue_w = torch.tensor([2, -3.4])true_b = 4.2# synthetic_data 用于生成 y = Xw + b + 噪声features, labels = d2l.synthetic_data(true_w, true_b, 1000)print(&#x27;features:&#x27;, features[0],&#x27;\\nlabel:&#x27;, labels[0]) 上面的代码中，因为 true_w 是二维的，所以对应生成的数据中 features 的每一行也是二维的数据样本。而 label 是一个一维的。（毕竟是在给定输入特征（输入和相应的特征构成数据集）的情况下预测标签。所以标签与 y 是相对应的） 123d2l.set_figsize()d2l.plt.scatter(features[:, (1)].detach().numpy(), labels.detach().numpy(), 1)d2l.plt.show() 这部分代码利用图像来表明 features 与 label 是线性关系。 读取数据集 123456789101112131415# 使之能够随机读取小批量样本def data_iter(batch_size, features, labels): num_examples = len(features) indices = list(range(num_examples)) # 这些样本是随机读取的，没有特定的顺序 random.shuffle(indices) for i in range(0, num_examples, batch_size): batch_indices = torch.tensor( indices[i: min(i + batch_size, num_examples)]) yield features[batch_indices], labels[batch_indices]batch_size = 10for X, y in data_iter(batch_size, features, labels): print(X, &#x27;\\n&#x27;, y) break 初始化模型参数 在下面的代码中，我们通过从均值为 0、标准差为 0.01 的正态分布中采样随机数来初始化权重，并将偏置初始化为 0。 ff 的小问题：为什么要使用正态分布来初始化权重 w 呢？ 使用正态分布来初始化权重 W 在神经网络训练中是非常常见的做法，主要原因是它能够帮助网络更快地收敛并提高训练效果。 避免对称性（通过使用正态分布随机初始化权重，可以破坏这种对称性，让每个神经元在训练中独立学习不同的特征。） 保持信号的适当尺度 保证初始激活值的分布合理 12w = torch.normal(0, 0.01, size=(2,1), requires_grad=True)b = torch.zeros(1, requires_grad=True) 定义模型 123def linreg(X, w, b): #@save &quot;&quot;&quot;线性回归模型&quot;&quot;&quot; return torch.matmul(X, w) + b 定义损失函数 我们需要将真实值 yyy 的形状转换为和预测值 y^\\hat{y}y^​ 的形状相同。 123def squared_loss(y_hat, y): #@save &quot;&quot;&quot;均方损失&quot;&quot;&quot; return (y_hat - y.reshape(y_hat.shape)) ** 2 / 2 定义优化算法 这边使用小批量随机梯度下降。 该函数接受模型参数集合、学习速率和批量大小作为输入。每一步更新的大小由学习速率 lr 决定。因为我们计算的损失是一个批量样本的总和，所以我们用批量大小（batch_size）来规范化步长，这样步长大小就不会取决于我们对批量大小的选择。 with torch.no_grad():在该模块下，所有 tensor 的requires_grad都被设置为 False，同时grad_fn也为 None。 123456def sgd(params, lr, batch_size): #@save &quot;&quot;&quot;小批量随机梯度下降&quot;&quot;&quot; with torch.no_grad(): for param in params: param -= lr * param.grad / batch_size param.grad.zero_() 这里的param.grad是由前面的l.sum().backward()（见下面代码）反向传播获得的，所以他这里其实计算的是一个平均损失。 训练 在每次迭代中，我们读取一小批量训练样本，并通过我们的模型来获得一组预测。计算完损失后，我们开始反向传播，存储每个参数的梯度。最后，我们调用优化算法 sgd来更新模型参数。 1234567891011121314151617lr = 0.03num_epochs = 3net = linregloss = squared_lossfor epoch in range(num_epochs): for X, y in data_iter(batch_size, features, labels): l = loss(net(X, w, b), y) # X和y的小批量损失 # 因为l形状是(batch_size,1)，而不是一个标量。l中的所有元素被加到一起， # 并以此计算关于[w,b]的梯度 l.sum().backward() sgd([w, b], lr, batch_size) # 使用参数的梯度更新参数 with torch.no_grad(): train_l = loss(net(features, w, b), labels) print(f&#x27;epoch &#123;epoch + 1&#125;, loss &#123;float(train_l.mean()):f&#125;&#x27;)print(f&#x27;w的估计误差: &#123;true_w - w.reshape(true_w.shape)&#125;&#x27;)print(f&#x27;b的估计误差: &#123;true_b - b&#125;&#x27;) 这里我因为对l.sum().backward()有点懵逼，所以去询问了一下 chatgpt（呜呜呜感谢 llm 解决我的简单小问题！） 为什么要用 l.sum().backward()？ 当我们使用批量数据时，损失 l 是一个向量。为了计算整体损失的梯度，我们需要将这个向量转换成标量。这就是 l.sum() 的作用。通过 l.sum()，我们将批量损失求和，得到一个标量，然后对这个标量进行反向传播，计算参数 w 和 b 的梯度。 梯度计算过程 损失函数： 均方损失函数的定义如下： loss=12n∑i=1n(yi−y^i)2 \\text{loss} = \\frac{1}{2n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 loss=2n1​i=1∑n​(yi​−y^​i​)2 其中 yiy_iyi​ 是实际值，y^i\\hat{y}_iy^​i​ 是预测值。 反向传播： 通过链式法则计算梯度。假设损失函数的结果是 l，那么对 l.sum() 进行反向传播，相当于对每个参数 w 和 b 进行如下计算： 对于权重 w： ∂loss∂w=∑i=1n∂loss∂y^i⋅∂y^i∂w\\frac{\\partial \\text{loss} }{\\partial w} = \\sum_{i=1}^{n} \\frac{\\partial \\text{loss} }{\\partial \\hat{y}_i} \\cdot \\frac{\\partial \\hat{y}_i}{\\partial w} ∂w∂loss​=i=1∑n​∂y^​i​∂loss​⋅∂w∂y^​i​​ 对于偏置 b： ∂loss∂b=∑i=1n∂loss∂y^i⋅∂y^i∂b\\frac{\\partial \\text{loss} }{\\partial b} = \\sum_{i=1}^{n} \\frac{\\partial \\text{loss} }{\\partial \\hat{y}_i} \\cdot \\frac{\\partial \\hat{y}_i}{\\partial b} ∂b∂loss​=i=1∑n​∂y^​i​∂loss​⋅∂b∂y^​i​​ 其中 y^i\\hat{y}_iy^​i​ 是模型的预测值，计算过程涉及到所有批量数据的损失。 线性回归的简洁实现 生成数据集 同上。 读取数据集 12345678def load_array(data_arrays, batch_size, is_train=True): #@save &quot;&quot;&quot;构造一个PyTorch数据迭代器&quot;&quot;&quot; dataset = data.TensorDataset(*data_arrays) return data.DataLoader(dataset, batch_size, shuffle=is_train)batch_size = 10data_iter = load_array((features, labels), batch_size)print(next(iter(data_iter))) 我们可以调用框架中现有的 API 来读取数据。我们将features和labels作为 API 的参数传递，并通过数据迭代器指定batch_size。此外，布尔值is_train表示是否希望数据迭代器对象在每个迭代周期内打乱数据。 定义模型 我们可以使用框架的预定义好的层。这使我们只需关注使用哪些层来构造模型，而不必关注层的实现细节。 我们首先定义一个模型变量net，它是一个Sequential类的实例。Sequential类将多个层串联在一起。当给定输入数据时，Sequential实例将数据传入到第一层，然后将第一层的输出作为第二层的输入，以此类推。(差不多Sequential类就是一个模型，然后把各个层按照顺序排好，使输入沿着一层一层直到输出。) 12from torch import nnnet = nn.Sequential(nn.Linear(2, 1)) 初始化模型参数 12net[0].weight.data.normal_(0, 0.01)net[0].bias.data.fill_(0) 定义损失函数 计算均方误差使用的是MSELoss类，也称为平方L2L_2L2​范数。 1loss = nn.MSELoss() 定义优化算法 小批量随机梯度下降算法是一种优化神经网络的标准工具，PyTorch在optim模块中实现了该算法的许多变种。当我们实例化一个SGD实例时，我们要指定优化的参数（可通过net.parameters()从我们的模型中获得）以及优化算法所需的超参数字典。小批量随机梯度下降只需要设置lr值，这里设置为 0.03。 1trainer = torch.optim.SGD(net.parameters(), lr=0.03) 训练 步骤依旧如下！对于各个小批量！ 通过调用net(X)生成预测并计算损失l（前向传播）。 通过进行反向传播来计算梯度。 通过调用优化器来更新模型参数。 1234567891011121314num_epochs = 3for epoch in range(num_epochs): for X, y in data_iter: l = loss(net(X) ,y) # 预测值, 实际值 trainer.zero_grad() # 清空梯度 l.backward() # 反向传播 trainer.step() # 更新！ l = loss(net(features), labels) print(f&#x27;epoch &#123;epoch + 1&#125;, loss &#123;l:f&#125;&#x27;)# 比较真实值和预测w，b的误差w = net[0].weight.dataprint(&#x27;w的估计误差：&#x27;, true_w - w.reshape(true_w.shape))b = net[0].bias.dataprint(&#x27;b的估计误差：&#x27;, true_b - b) softmax 回归 分类问题 表示分类数据的简单方法：独热编码（one‐hot encoding）。独热编码是一个向量，它的分量和类别一样多。类别对应的分量设置为 1，其他所有分量设置为 0。 网络架构 为了解决线性模型的分类问题，我们需要和输出一样多的仿射函数（affine function）。每个输出对应于它自己的仿射函数。 比如说，我们要区分猫、鸡、狗，假设每次输入是一个 2 × 2 的灰度图像。我们可以用一个标量表示每个像素值，每个图像对应四个特征x1*, x2, x3, x4。由于我们有 4 个特征和 3 个可能的输出类别，我们将需要 12 个标量来表示权重（带下标的w），3 个标量来表示偏置（带下标的*b*）。然后，我们可以写出为规范的输出与输入的函数式如下： o1=x1w11+x2w12+x3w13o2=x1w21+x2w22+x3w23o3=x1w31+x2w32+x3w33o_1 = x_1w_{11} + x_2w_{12} + x_3w_{13} \\\\ o_2 = x_1w_{21} + x_2w_{22} + x_3w_{23} \\\\ o_3 = x_1w_{31} + x_2w_{32} + x_3w_{33} o1​=x1​w11​+x2​w12​+x3​w13​o2​=x1​w21​+x2​w22​+x3​w23​o3​=x1​w31​+x2​w32​+x3​w33​ 与线性回归一样，softmax 回归也是一个单层神经网络. 全连接层的参数开销 具体来说，对于任何具有d个输入和q个输出的全连接层，参数开销为O(dq)。 幸运的是，将d个输入转换为q个输出的成本可以减少到O(dqn)O(\\frac{dq}{n})O(ndq​)，其中超参数n可以由我们灵活指定。 softmax 运算 优化参数以最大化观测数据的概率并希望模型的输出yj^\\hat {y_j}yj​^​可以视为属于类 j 的概率。 即如果y1^\\hat{y_1}y1​^​、y2^\\hat{y_2}y2​^​、y3^\\hat{y_3}y3​^​分别为 0.1、0.8 和 0.1，那么即使 2 对应的类型。 要将输出视为概率，我们必须保证在任何数据上的输出都是非负的且总和为 1。 我们需要一个训练的目标函数，来激励模型精准地估计概率。例如，在分类器输出 0.5 的所有样本中，我们希望这些样本是刚好有一半实际上属于预测的类别。这个属性叫做校准（calibration）。 softmax 函数能够将未规范化的预测变换为非负数并且总和为 1，同时让模型保持可导的性质。 非负数：对每个oio_ioi​求幂 总和为 1：让每个求幂后的结果除以它们的总和 y^=softmax(o) 其中 yj^=exp(oj)∑kexp(ok)\\mathbf{\\hat y} = softmax(\\mathbf o) \\space \\text{其中} \\space \\hat{y_j} = \\frac{exp(o_j)}{\\sum_kexp(o_k)} y^​=softmax(o) 其中 yj​^​=∑k​exp(ok​)exp(oj​)​ 然后我们就可以根据其数值来判断属于哪个类别啦！ 尽管 softmax 是一个非线性函数，但 softmax 回归的输出仍然由输入特征的仿射变换决定。因此，softmax 回归是一个线性模型（linear model）。 小批量样本的矢量化 softmax 的矢量表达式为： O=XW+bY^=softmax(O)\\mathbf O = \\mathbf{XW} + \\mathbf b \\\\ \\mathbf{\\hat Y} = softmax(\\mathbf O) O=XW+bY^=softmax(O) 由于X中的每一行代表一个数据样本，那么 softmax 运算可以按行（rowwise）执行：对于O的每一行，我们先对所有项进行幂运算，然后通过求和对它们进行标准化。 损失函数 对数似然：softmax 函数给出了一个向量y^\\hat{y}y^​ ，我们可以将其视为“对给定任意输入x的每个类的条件概率”。 −log P(Y∣X)=∑i=1n−log P(y(i)∣x(i))=∑i=1nl(y(i),y^(i))-log\\space P(\\mathbf Y | \\mathbf X) = \\sum_{i=1}^n-log\\space P(\\mathbf y^{(i)}|\\mathbf x^{(i)}) = \\sum_{i=1}^nl(\\mathbf y^{(i)}, \\mathbf{\\hat y}^{(i)}) −log P(Y∣X)=i=1∑n​−log P(y(i)∣x(i))=i=1∑n​l(y(i),y^​(i)) 交叉熵损失（它是所有标签分布的预期损失值）：由于 y 是一个长度为 q 的独热编码向量，所以除了一个项以外的所有项 j 都消失了。 l(y,y^)=−∑j=1qyjlog yj^l(\\mathbf y, \\hat{\\mathbf y}) = -\\sum_{j=1}^qy_jlog\\space \\hat{y_j} l(y,y^​)=−j=1∑q​yj​log yj​^​ 交叉熵从P到Q，记为H(P, Q)。我们可以把交叉熵想象为“主观概率为Q的观察者在看到根据概率P生成的数据时的预期惊异”。当P = Q时，交叉熵达到最低。在这种情况下，从P到Q的交叉熵是H(P, P) = H(P)。 图像分类数据集 读取 fashion-mnist 的数据，然后进行小批量读取数据， 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051import torchimport torchvisionfrom torch.utils import datafrom torchvision import transformsfrom d2l import torch as d2ld2l.use_svg_display()# 读取数据集# 通过ToTensor实例将图像数据从PIL类型变换成32位浮点数格式，# 并除以255使得所有像素的数值均在0～1之间trans = transforms.ToTensor()mnist_train = torchvision.datasets.FashionMNIST(root=&quot;../data&quot;, train=True, transform=trans, download=True)mnist_test = torchvision.datasets.FashionMNIST(root=&quot;../data&quot;, train=False, transform=trans, download=True)print(len(mnist_train), len(mnist_test))print(mnist_train[0][0].shape)def get_fashion_mnist_labels(labels): #@save &quot;&quot;&quot;返回Fashion-MNIST数据集的文本标签&quot;&quot;&quot; text_labels = [&#x27;t-shirt&#x27;, &#x27;trouser&#x27;, &#x27;pullover&#x27;, &#x27;dress&#x27;, &#x27;coat&#x27;, &#x27;sandal&#x27;, &#x27;shirt&#x27;, &#x27;sneaker&#x27;, &#x27;bag&#x27;, &#x27;ankle boot&#x27;] return [text_labels[int(i)] for i in labels]X, y = next(iter(data.DataLoader(mnist_train, batch_size=18)))d2l.show_images(X.reshape(18, 28, 28), 2, 9, titles=get_fashion_mnist_labels(y))d2l.plt.show()# 小批量读取数据batch_size = 256def get_dataloader_workers(): #@save &quot;&quot;&quot;使用4个进程来读取数据&quot;&quot;&quot; return 4train_iter = data.DataLoader(mnist_train, batch_size, shuffle=True, num_workers=get_dataloader_workers())timer = d2l.Timer()for X, y in train_iter: continueprint(f&#x27;&#123;timer.stop():.2f&#125; sec&#x27;)&quot;&quot;&quot;load_data_fashion_mnist函数，用于获取和读取Fashion‐MNIST数据集。这个函数返回训练集和验证集的数据迭代器。此外，这个函数还接受一个可选参数resize，用来将图像大小调整为另一种形状。&quot;&quot;&quot;train_iter, test_iter = d2l.load_data_fashion_mnist(32, resize=32)for X, y in train_iter: print(X.shape, X.dtype, y.shape, y.dtype) break softmax 回归的从零开始实现 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859import torchfrom IPython import displayfrom d2l import torch as d2lbatch_size = 256train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)# 初始化模型参数num_inputs = 784num_outputs = 10W = torch.normal(0, 0.01, size=(num_inputs, num_outputs), requires_grad=True)b = torch.zeros(num_outputs, requires_grad=True)# 定义softmax操作X = torch.tensor([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])# 当调用sum运算符时，我们可以指定保持在原始张量的轴数，而不折叠求和的维度。print(X.sum(0, keepdim=True), X.sum(1, keepdim=True))def softmax(X): X_exp = torch.exp(X) partition = X_exp.sum(1, keepdim=True) return X_exp / partition # 这里应用了广播机制# 这里的(2,5)是输出维度X = torch.normal(0, 1, (2, 5))X_prob = softmax(X)print(X_prob, X_prob.sum(1))# 定义模型def net(X): return softmax(torch.matmul(X.reshape((-1, W.shape[0])), W) + b)# 定义损失函数y = torch.tensor([0, 2])y_hat = torch.tensor([[0.1, 0.3, 0.6], [0.3, 0.2, 0.5]])y_hat[[0, 1], y]# 交叉熵损失函数def cross_entropy(y_hat, y): # y_hat[range(len(y_hat)), y] 选择每个样本对应的正确类别的预测概率。例如，对于第 i 个样本，它选择 y_hat[i, y[i]]。 return - torch.log(y_hat[range(len(y_hat)), y])print(cross_entropy(y_hat, y))# 分类精度print(d2l.accuracy(y_hat, y) / len(y))# 评估在模型的精度def evaluate_accuracy(net, data_iter): #@save &quot;&quot;&quot;计算在指定数据集上模型的精度&quot;&quot;&quot; if isinstance(net, torch.nn.Module): net.eval() # 将模型设置为评估模式 metric = d2l.Accumulator(2) # 正确预测数、预测总数 with torch.no_grad(): for X, y in data_iter: metric.add(d2l.accuracy(net(X), y), y.numel()) return metric[0] / metric[1]# if __name__ == &#x27;__main__&#x27;:# print(&quot;accuracy:&quot;, evaluate_accuracy(net, test_iter))# 训练lr = 0.1def updater(batch_size): return d2l.sgd([W, b], lr, batch_size)num_epochs = 10d2l.train_ch3(net, train_iter, test_iter, cross_entropy, num_epochs, updater)d2l.predict_ch3(net, test_iter)d2l.plt.show() 训练 softmax 回归循环模型与训练线性回归模型非常相似：先读取数据，再定义模型和损失函数，然后使用优化算法训练模型。大多数常见的深度学习模型都有类似的训练过程。 softmax回归的简洁实现 读取数据和初始化 123456789101112import torchfrom torch import nnfrom d2l import torch as d2lbatch_size = 256train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)# PyTorch不会隐式地调整输入的形状。因此，# 我们在线性层前定义了展平层（flatten），来调整网络输入的形状net = nn.Sequential(nn.Flatten(), nn.Linear(784, 10))def init_weights(m): if type(m) == nn.Linear: nn.init.normal_(m.weight, std=0.01)net.apply(init_weights) nn.Flatten()的作用：将多维输入张量展平为二维张量，使其可以输入到线性层（全连接层）中。 在图像分类等任务中，输入数据通常是多维张量。例如，对于手写数字识别任务（如 MNIST 数据集），每个输入图像的形状是 (batch_size, 1, 28, 28)，其中： batch_size 是批次大小。 1 是通道数（灰度图像的通道数为 1）。 28 是图像的高度和宽度（MNIST 图像大小为 28x28）。 nn.Flatten() 的作用是将多维输入张量展平为二维张量。具体地，它将形状 (batch_size, channels, height, width) 的张量展平成形状 (batch_size, channels * height * width) 的张量。 net.apply()将 init_weights 函数应用到网络的每一层，确保线性层的权重被正确初始化。 重新审视 Softmax 的实现 （这部分的语言好有意思！咳咳，你也不想看到因为上溢导致的 inf 或者下溢以后反向传播导致的 nan 叭！） 避免上溢，给每个值都减去最大的值。 \\hat{y_j} &amp;= &amp;\\frac{exp(o_j-max(o_k))exp(max(o_k))}{\\sum_k exp(o_k-max(o_k))exp(max(o_k))}\\\\ &amp;=&amp;\\frac{exp(o_j-max(o_k))}{\\sum_k exp(o_k-max(o_k))} 上面这个只解决了上溢的问题，但是呢，这么做会产生很大的负数，在反向传播中可能会出现好多 nan，然后就取 log！这种方法利用了“LogSumExp”技巧。 (下面这部分是 chatgpt 教的！因为原文总感觉缺点东西！不过和原文还是有点区别的！但是思想是一致的！)具体做法是： 计算 logits 的对数和指数部分。 结合计算 softmax 和交叉熵损失。 交叉熵损失可以重写为： CrossEntropy(y,y^)=−∑iyilog⁡(exp⁡(zi)∑jexp⁡(zj))CrossEntropy(y, \\hat{y}) = -\\sum_{i} y_i \\log \\left( \\frac{\\exp(z_i)}{\\sum_{j} \\exp(z_j)} \\right)CrossEntropy(y,y^​)=−∑i​yi​log(∑j​exp(zj​)exp(zi​)​) 将上式展开： CrossEntropy(y,y^)=−∑iyi(zi−log⁡(∑jexp⁡(zj)))CrossEntropy(y, \\hat{y}) = -\\sum_{i} y_i (z_i - \\log(\\sum_{j} \\exp(z_j)))CrossEntropy(y,y^​)=−∑i​yi​(zi​−log(∑j​exp(zj​))) 这里，我们引入了“LogSumExp”技巧： log⁡∑jexp⁡(zj)\\log\\sum_{j} \\exp(z_j)log∑j​exp(zj​) 这一技巧可以简化为 PyTorch 中的 torch.nn.CrossEntropyLoss 函数，它直接接受未规范化的 logits，并在内部计算 softmax 及其对数，从而提高数值稳定性。 12loss = nn.CrossEntropyLoss(reduction=&#x27;none&#x27;)# reduction=&#x27;none&#x27; 表示不对损失值进行聚合，返回每个样本的损失值。这在需要更灵活地处理损失时非常有用，例如分析每个样本的损失或对损失进行加权处理。 优化与训练 123trainer = torch.optim.SGD(net.parameters(), lr=0.1)num_epochs = 10d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer) ff 的一些小感悟 首先是感谢 chatgpt 的优秀回答呜呜呜！希望 ff 以后也能为做出这么优秀或者更加优秀的东西做贡献！ 然后真的感觉还是要静下心来学习。感觉之前看得迷迷糊糊得地方，这次看真的通顺好多了。 以及以及！周日摆烂了！结果今天才把线性回归的部分看完！后面也要继续加油啊！","categories":[{"name":"学习笔记","slug":"学习笔记","permalink":"https://yxf203.github.io/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"}],"tags":[{"name":"动手学深度学习","slug":"动手学深度学习","permalink":"https://yxf203.github.io/tags/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"}]},{"title":"预备知识","slug":"预备知识","date":"2024-07-13T02:57:29.000Z","updated":"2024-07-13T02:57:29.672Z","comments":true,"path":"2024/07/13/预备知识/","link":"","permalink":"https://yxf203.github.io/2024/07/13/%E9%A2%84%E5%A4%87%E7%9F%A5%E8%AF%86/","excerpt":"","text":"这部分之前看的时候好像只简单的过了一遍~但是这次还是想认真看一下，毕竟好多知识确实都忘记了。 数据操作 张量 ​ 张量（tensor），是n维数字，类似于numpy中的ndarray.而区别点在于：tensor支持GPU加速计算，并且支持自动微分（这在机器学子中很实用）。 入门 123456789import torchx = torch.arrange(12) # 创建一个从0开始，有12个元素的张量（行向量）torch([0,1,2,3,..,11])x.shape # 获取张量（沿各个轴的长度）的形状 torch.Size([12])x.numel() # 获取张量中元素个数X = x.reshape(3, 4) # 字面意思 可以利用-1来自动计算，即可写成 x.reshape(-1, 4)或者x.reshape(3, -1) 剩下的值会自动计算torch.zeros((2,3,4)) # 字面意思torch.ones((2,3,4)) # 字面意思torch.randn(3, 4) # 产生3*4的数组，每个元素均从均值为0，标准差未1的标准高斯分布（即正态分布）中采样torch.tensor([[2, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]]) # 字面意思 运算符 对于二元运算符，c=F(u,v)\\pmb{c} = F(\\pmb{u}, \\pmb{v})cc=F(uu,vv) 的具体计算方法是ci←f(ui,vi)c_i \\leftarrow f(u_i, v_i)ci​←f(ui​,vi​), 其中cic_ici​、uiu_iui​和viv_ivi​分别是向量c、u和v中的元素。 12345678910111213141516x = torch.tensor([1.0, 2, 4, 8])y = torch.tensor([2, 2, 2, 2])x + y, x - y, x * y, x / y, x ** ytorch.exp(x) # 字面意思+上面的定义X = torch.arange(12, dtype=torch.float32).reshape((3,4))Y = torch.tensor([[2.0, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])# 将多个张量concatenate在一起，dim指定的是沿哪个轴concatenate 然后他这个索引从0开始，顺序是指定形状的那个顺序，差不多是从外向内torch.cat((X, Y), dim=0), torch.cat((X, Y), dim=1)# 对这个输出结果的解释，第一个输出结果，沿dim=0 concatenate，是最外侧进行concatenate，本来两个都是3*4，最外侧，或者索引为0是3这个，连结以后变成6*4# dim=1同上，最后得到3*8# 测试了一下，这个要是二者形状不同会报错呢 比如说2*2和1*4是会报错的X == Y # 相应位置的元素相同则为True，不同为FalseX.sum() # 字面意思，产生的依旧是张量 广播机制 上面举的例子是相同形状的，而广播机制可以在某些情况下对不同形状的张量进行元素操作。机制如下： 通过适当复制元素来扩展一个或两个数组，以便在转换之后，两个张量具有相同形状。 对生成的数组执行按元素操作。 1234567a = torch.arange(3).reshape((3, 1))b = torch.arange(2).reshape((1, 2))a + b # tensor([[0, 1],# [1, 2],# [2, 3]])# 从3*1和1*2变成了3*2 索引与切片 和python基本语法类似。 123X[-1], X[1:3]X[1, 2] = 9X[0:2, :] = 12 节省内存 执行原地操作从而实现节省内存的需求，而原地操作只需要Z[:] = &lt;expression&gt;. 12345# 该代码可验证使用该方式是不开新的内存的Z = torch.zeros_like(Y)print(&#x27;id(Z):&#x27;, id(Z))Z[:] = X + Yprint(&#x27;id(Z):&#x27;, id(Z)) 转换为其他python对象 123456# numpy的ndarray转换为tensorA = X.numpy()B = torch.tensor(A)# tensor转换为其他的a = torch.tensor([3.5])a, a.item(), float(a), int(a) 数据预处理 读取数据集 12345678910111213import osos.makedirs(os.path.join(&#x27;.&#x27;, &#x27;data&#x27;), exist_ok=True)data_file = os.path.join(&#x27;.&#x27;, &#x27;data&#x27;, &#x27;house_tiny.csv&#x27;)with open(data_file, &#x27;w&#x27;) as f: f.write(&#x27;NumRooms,Alley,Price\\n&#x27;) # 列名 f.write(&#x27;NA,Pave,127500\\n&#x27;) # 每行表示一个数据样本 f.write(&#x27;2,NA,106000\\n&#x27;) f.write(&#x27;4,NA,178100\\n&#x27;) f.write(&#x27;NA,NA,140000\\n&#x27;) import pandas as pddata = pd.read_csv(data_file)print(data) 处理缺失值 典型的方法：插值法（用一个替代值弥补）和删除法（直接忽略缺失值）。 12345678# 对数字列，也就是NumRooms的NaN用平均值填充inputs, outputs = data.iloc[:, 0:2], data.iloc[:, 2]inputs = inputs.fillna(inputs.mean())print(inputs)# 将Alley（因为该列只有&quot;Pave&quot;和&quot;NaN&quot;）分为&quot;Alley_Pave&quot;和&quot;Alley_nan&quot;,分别设置为0和1# get_dummies在对变量进行独热编码时使用inputs = pd.get_dummies(inputs, dummy_na=True)print(inputs) 转换为张量格式 1234import torch# 这个和前面的转换为张良格式的方法是一样的，不过要把dataframe格式转换成numpy中的ndarrayX = torch.tensor(inputs.to_numpy(dtype=float))y = torch.tensor(outputs.to_numpy(dtype=float)) 线性代数 标量 标量由只有一个元素的张量表示。（其余的与我们在数学中接触的标量没有太大区别） 向量 标量值组成的列表。 使用一维张量表示。 矩阵 123A = torch.arange(20).reshape(5, 4)# 矩阵的转置 对任意i,j，都有bij=aji.A.T 张量 描述任意数量轴的n维数组。 降维 1234567891011A = torch.arange(20, dtype=torch.float32).reshape(5, 4)# 表示任意形状张量的元素和。 sum()是将所有元素加和A.shape, A.sum()# 可以指定按哪个轴来通过求和降低维度A_sum_axis0 = A.sum(axis=0)A_sum_axis0, A_sum_axis0.shape# 沿着行和列对矩阵求和，等价于对矩阵的所有元素进行求和。A.sum(axis=[0, 1]) # 结果和A.sum()相同# 求平均值A.mean(), A.sum() / A.numel()A.mean(axis=0), A.sum(axis=0) / A.shape[0] 非降维 1234567# 保持轴数必变，但按照列进行了求和，但是依旧是二维，没有向上面一样变成一维向量sum_A = A.sum(axis=1, keepdims=True)sum_A# 通过广播将A除以sum_A (这有利于正则化？大概是叫这个)A / sum_A# 沿某个轴计算累计总和A.cumsum(axis=0) 点积（Dot Product） 和数学物理上的点乘是一样的，也是对应位置的元素进行相乘，点积结束后也往往得到一个标量。 12345x = torch.arange(4, detype = torch.float32)y = torch.ones(4, dtype = torch.float32)x, y, torch.dot(x, y)# 也可以通过执行按元素乘法，然后进行求和来表示两个向量的点积torch.sum(x * y) 矩阵-向量积 A∈Rm×nA \\in R^{m\\times n}A∈Rm×n, x∈Rnx \\in R^nx∈Rn，可以将RnR^nRn转换为RmR^mRm 12# 在代码中使用张量表示矩阵-向量积，我们使用mv函数A.shape, x.shape, torch.mv(A, x) 矩阵-矩阵乘法 123B = torch.ones(4, 3)# 使用mm函数来进行矩阵-矩阵乘法torch.mm(A, B) 范数 线性代数中最有用的一些运算符是范数（norm）。非正式地说，向量的范数是表示一个向量有多大。这里考虑的大小（size）概念不涉及维度，而是分量的大小。 线性代数中，向量范数是将向量映射到标量的函数fff.其需要满足 f(αx)=∣α∣f(x)f(x+y)≤f(x)+f(y)f(x)≥0f(\\alpha \\pmb x) = |\\alpha|f(\\pmb x)\\\\ f(\\pmb x + \\pmb y) \\leq f(\\pmb x) + f(\\pmb y) \\\\ f(\\pmb x) \\geq 0 f(αxx)=∣α∣f(xx)f(xx+yy)≤f(xx)+f(yy)f(xx)≥0 Lp范数：∣∣X∣∣p=(∑i=1n∣xi∣p)1/p||\\pmb X||_p = (\\sum_{i=1}^{n} |x_i|^p)^{1/p}∣∣XX∣∣p​=(∑i=1n​∣xi​∣p)1/p 12345u = torch.tensor([3.0, -4.0])# 计算u的L2范数torch.norm(u)# 计算u的L1范数torch.abs(u).sum() 矩阵X∈Rm×n\\pmb X \\in R^{m \\times n}XX∈Rm×n的Frobenius范数是矩阵元素平方和的平方根 ∣∣X∣∣=∑i=1m∑j=1nxij2||\\pmb X|| = \\sqrt{\\sum_{i=1}^{m} \\sum_{j=1}^{n}x_{ij}^2}∣∣XX∣∣=∑i=1m​∑j=1n​xij2​​ torch.norm(torch.ones((4, 9)))计算矩阵的Frobenius范数。 微积分 绘图显示！ 123456import matplotlib.pyplot as plt# 在d2l.plot之后！plt.show()# 或者也可以直接d2l.plt.show() 梯度(这里有点不太理解！后面如果遇到了问题的话请回来看！并了解其推导过程！) ∇xf(x)=[∂f(x)∂x1,∂f(x)∂x2,...,∂f(x)∂x2]T\\nabla_x f(\\pmb x) = \\left[ \\frac{\\partial f(\\pmb x)}{\\partial x_1}, \\frac{\\partial f(\\pmb x)}{\\partial x_2}, ..., \\frac{\\partial f(\\pmb x)}{\\partial x_2} \\right]^T ∇x​f(xx)=[∂x1​∂f(xx)​,∂x2​∂f(xx)​,...,∂x2​∂f(xx)​]T 没有歧义时，用∇f(x)\\nabla f(\\pmb x)∇f(xx)替代。 假设x\\pmb xxx维n维向量，在微分多元函数时经常使用以下规则： 对于所有A∈Rm×n\\pmb A \\in R^{m \\times n}AA∈Rm×n, 都有∇xAx=AT\\nabla\\pmb{_x}\\pmb{Ax} = \\pmb A^T∇x​x​AxAx=AAT 对于所有A∈Rm×n\\pmb A \\in R^{m \\times n}AA∈Rm×n, 都有∇xxTA=A\\nabla\\pmb{_x}\\pmb{x^TA} = \\pmb A∇x​x​xTAxTA=AA 对于所有A∈Rm×n\\pmb A \\in R^{m \\times n}AA∈Rm×n, 都有∇xxTAx=(A+AT)x\\nabla\\pmb{_x}\\pmb{x^TAx} = \\pmb{(A+A^T)x}∇x​x​xTAxxTAx=(A+AT)x(A+AT)x 自动微分 系统会构建一个计算图（computational graph），来跟踪计算式哪些数据通过哪些操作组合起来产生输出。 自动微分使系统能够随后反向传播梯度。这里，**反向传播（backpropagate）**意味着跟踪整个计算图，填充关于每个参数的偏导数。 123456789101112131415import torchx = torch.arange(4.0)print(torch.dot(x, x))x.requires_grad_(True) # 等价于x=torch.arange(4.0,requires_grad=True)print(x.grad) # 默认值是Noney = 2 * torch.dot(x, x)print(y)# 通过反向传播来自动计算y关于x每个分量的梯度，并打印这些梯度。y.backward()print(x.grad)# 在默认情况下，PyTorch会累积梯度，我们需要清除之前的值x.grad.zero_()y = x.sum()y.backward()print(x.grad) 一些借助chatgpt的小理解！ torch.dot(x, x) 的计算过程 假设 x = torch.arange(4.0, requires_grad=True)，那么 x 包含的值是 [0.0, 1.0, 2.0, 3.0]。计算 torch.dot(x, x) 相当于计算 x 的点积，即： f(x)=x02+x12+x22+x32f(\\mathbf{x}) = x_0^2 + x_1^2 + x_2^2 + x_3^2f(x)=x02​+x12​+x22​+x32​ 梯度计算 反向传播时，计算的是函数f(x)f(\\mathbf{x})f(x) 对每个分量 xix_ixi​ 的梯度，即偏导数。 非标量变量的反向传播 1234567# 对非标量调用backward需要传入一个gradient参数，该参数指定微分函数关于self的梯度。# 本例只想求偏导数的和，所以传递一个1的梯度是合适的x.grad.zero_()y = x * x# 等价于y.backward(torch.ones(len(x)))y.sum().backward()x.grad gradient 参数的作用 在反向传播中，gradient 参数指定了每个输出元素的梯度初值。对于标量输出，这个初值通常是1；对于向量或张量输出，需要提供一个与其形状相同的张量，来指示每个元素的梯度。 初值通常是1：当输出是标量时，初始梯度值是1，因为这是链式法则计算中的标准初始值。 非标量输出时的自定义初值：对于非标量输出，我们可以通过传递一个自定义的初始梯度张量来调整每个分量的权重，从而实现不同的梯度计算需求。 分离计算 将某些计算移动到记录的计算图之外。 12345678x.grad.zero_()y = x * x# 将y从计算图中分离出去，u是一个与y具有相同的值，但丢弃计算图中如何计算y的任何信息的变量u = y.detach()# 因为y从图中分离，因此求得不再是z=x*x*x的偏导数，而是将u看作常量，z=u*x的偏导数z = u * xz.sum().backward()x.grad == u 概率 联合概率： $P(A = a, B=b) ,即,即,即A=a和和和B=b$同时满足的概率。 条件概率：P(B=b∣A=a)P(B=b|A=a)P(B=b∣A=a)表示。 贝叶斯定理： P(A,B)=P(B∣A)P(A)P(A,B) = P(B|A)P(A)P(A,B)=P(B∣A)P(A) 与 P(A,B)=P(A∣B)P(B)P(A,B) = P(A|B)P(B)P(A,B)=P(A∣B)P(B) 可以推出 P(A∣B)=P(B∣A)P(A)P(B)P(A|B)=\\frac{P(B|A)P(A)}{P(B)} P(A∣B)=P(B)P(B∣A)P(A)​ 边际化 独立性","categories":[{"name":"学习笔记","slug":"学习笔记","permalink":"https://yxf203.github.io/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"}],"tags":[{"name":"动手学深度学习","slug":"动手学深度学习","permalink":"https://yxf203.github.io/tags/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"}]},{"title":"引言","slug":"引言","date":"2024-07-11T11:14:39.000Z","updated":"2024-07-11T11:14:39.577Z","comments":true,"path":"2024/07/11/引言/","link":"","permalink":"https://yxf203.github.io/2024/07/11/%E5%BC%95%E8%A8%80/","excerpt":"","text":"虽然之前也看过一些，但是到现在已经过去了好久了，期间也没再看过这本书。 李沐大神的《动手学深度学习》的评价一直都很高，但是内容真的好多。 希望这次能努力坚持着多学一点内容。 那么！ff的从零开始学习！启动！ 机器学习（machine learning，ML）是一类强大的可以从经验中学习的技术. 只需要定义一个灵活的程序算法，其输出由许多参数（parameter）决定，然后使用数据集来确定当下的“最佳参数集”. 任一调整参数后的程序被称为模型（model）。 使用数据集来选择参数的元程序被称为学习算法（learning algorithm）。 深度学习与经典方法的区别主要在于：前者关注的功能强大的模型，这些模型由神经网络错综复杂的交织在一起，包含层层数据转换，因此被称为深度学习（deep learning）。 机器学习中的关键组件 可以用来学习的数据(当每个样本的特征类别数量都是相同的时候，其特征向量是固定长度的，这个长度被称为数据的维数（di‐mensionality）。) 如何转换数据的模型 一个目标函数，用来量化模型的有效性（目标函数：定义模型的优劣程度的度量，这个度量在大多数情况可视化。因为我们通常定义一个目标函数并希望其越低越好，所以又是又称他们为损失函数（loss function ,or cost function.））. 调整模型参数以优化目标函数的算法 监督学习：在给定输入特征（输入和相应的特征构成数据集）的情况下预测标签。 回归：标签是一个数值。 分类：解决“哪一个”的问题。 标签：使机器能够描绘，一个事物可能有多个标签。 搜索（首先为集合中的每个元素分配相应的相关性分数，然后检索评级最高的元素。）如今，搜索引擎使用机器学习和用户行为模型来获取网页相关性得分。 推荐系统 序列学习：使机器具有“记忆”功能 无监督学习（不含有“目标”） 聚类（clustering）问题：没有标签的情况下 主成分分析（principal component analysis）问题：找到少量的参数来准确地捕捉数据的线性相关属性 因果关系（causality）和概率图模型（probabilistic graphical models）问题：根本原因，根据经验数据发现它们之间的关系。 生成对抗性网络（generative adversarial networks）: 合成数据的方法。 与环境互动 监督学习和无监督学习都是一种离线学习，因为他们利用从环境中得到的数据，在与环境断开的情况下，孤立地进行模式识别。但是，这也决定了他们解决问题的局限性和有限性。 而与真实环境互动则可能会去影响环境。这样的人工智能就是一种智能代理而不是预测模型了。 强化学习（为了产生一个好的策略） 必须处理学分分配（credit assignment）问题 大概粗略地把引言看了一看，怎么说呢，突然感觉好像也并没有那么糟糕。 看到自动驾驶汽车，看到Alphago胜利的例子，发现原来以前觉得遥不可及的东西我也在慢慢地接近，尽管依旧路漫漫其修远兮。好像到今为止大家都在抨击AI（这就不详细区分到底是机器学习还是什么部分啦），但是实际的生活中的变化却也在切切实实地证明着，这个领域有其独到之处，也在努力地推动人们生活或者其它领域的发展。这样子想着，好像不管是做理论也好还是做应用也好对我而言都是有意义的。或许我的理论确实微不足道，但是也正是很多微不足道的理论才促进了最后宏伟理论地产生。毕竟，是时势造英雄啊。 关于练习这一部分，看到第一个！ 你当前正在编写的代码的哪些部分可以“学习”，即通过学习和自动确定代码中所做的设计选择来改进？你的代码是否包含启发式设计选择？ 这个确确实实已经在pycharm或者IDEAJ等编译器里投入实际应用了，我觉得也是辅助大家写代码的一步把。 然后这边的概念端到端的训练方法我还是不太理解，这边先mark一下，希望之后看到的时候能够想起来！","categories":[{"name":"学习笔记","slug":"学习笔记","permalink":"https://yxf203.github.io/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"}],"tags":[{"name":"动手学深度学习","slug":"动手学深度学习","permalink":"https://yxf203.github.io/tags/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"}]},{"title":"悬线法","slug":"悬线法","date":"2024-07-11T03:59:43.000Z","updated":"2024-07-11T07:33:43.062Z","comments":true,"path":"2024/07/11/悬线法/","link":"","permalink":"https://yxf203.github.io/2024/07/11/%E6%82%AC%E7%BA%BF%E6%B3%95/","excerpt":"","text":"好，又是这道题（指洛谷P4147玉蟾宫）。 继被其单调栈的解法摧残以后，现在又来被其悬线法的解法摧残了。不过悬线法的话直接套模板其实挺简单的，思路也比直接使用单调栈要来的简单多。 然后悬线法其实是一种dp思想的体现好像，所以归到dp里了。 悬线法 悬线法就是先用向上的射线得到一条绳子，然后左右摆来摆去得到极大矩形，最后获得最大矩形。是用来求最大矩形的一种较为好理解和简单的方式。 悬线法的适用范围是单调栈的子集。具体来说，悬线法可以应用于满足以下条件的题目（摘自 oi-wiki）： 需要在扫描序列时维护单调的信息； 可以使用单调栈解决； 不需要在单调栈上二分。 定义 lil_ili​为当前找到的iii位置的悬线能扩展到的最左边的位置，容易得到lil_ili​初始为iii，我们需要进一步判断还能不能进一步往左扩展。 如果当前 li=1l_i = 1li​=1，则已经扩展到了边界，不可以。 如果当前 ai&gt;ali−1a_i &gt; a_{l_i-1}ai​&gt;ali​−1​，则从当前悬线扩展到的位置不能再往左扩展了。 如果当前 ai&lt;=ali−1a_i &lt;= a_{l_i-1}ai​&lt;=ali​−1​，则从当前悬线还可以往左扩展，并且li−1l_i - 1li​−1位置的悬线能向左扩展到的位置，iii位置的悬线一定也可以扩展到，于是我们将lil_ili​更新为lli−1l_{l_i-1}lli​−1​，并继续执行判断。 注意是lli−1l_{l_i-1}lli​−1​哦！ 模板 12345for (int j = 1; j &lt;= m; j++) while (l[j] != 1 &amp;&amp; a[l[j] - 1] &gt;= a[j]) l[j] = l[l[j] - 1];for (int j = m; j &gt;= 1; j--) while (r[j] != m &amp;&amp; a[r[j] + 1] &gt;= a[j]) r[j] = r[r[j] + 1];for (int j = 1; j &lt;= m; j++) ans = std::max(ans, (r[j] - l[j] + 1) * a[j]); 不过在此之前记得给 l 数组和 r 数组赋值（索引值）。 玉蟾宫题解 123456789101112131415161718192021222324252627282930313233#include&lt;bits/stdc++.h&gt;using namespace std;char a[1010][1010];int l[1010], r[1010];int main()&#123; int n, m, i, j, max_n = 0, temp; int count[100][100]; memset(count, 0, sizeof(count)); cin&gt;&gt;n&gt;&gt;m; for(i = 1; i &lt;= n; i++)&#123; for(j = 1; j &lt;= m; j++)&#123; cin&gt;&gt;a[i][j]; &#125; &#125; for(i = 1; i &lt;= n; i++)&#123; for(j = 1; j &lt;= m; j++)&#123; l[j] = r[j] = j; count[i][j] = 0; if(a[i][j] == &#x27;F&#x27;) count[i][j] = count[i - 1][j] + 1; &#125; for(j = 1; j &lt;= m; j++)&#123; while(l[j] &gt; 1 &amp;&amp; count[i][j] &lt;= count[i][l[j] - 1]) l[j] = l[l[j] - 1]; &#125; for(j = m; j &gt;= 1; j--)&#123; while(r[j] &gt; 1 &amp;&amp; count[i][j] &lt;= count[i][r[j] + 1]) r[j] = r[r[j] + 1]; &#125; for(j = 1; j &lt;= m; j++)&#123; max_n = max(max_n, count[i][j] * (r[j] - l[j] + 1)); &#125; &#125; cout&lt;&lt;max_n * 3&lt;&lt;endl; return 0;&#125; 一些学习记录 其实oi-wiki上的题解更为简洁！然后空间消耗也更小！ 比如说我这边count是开了二维数组，但是oi-wiki上的解法只开了一个一位数组就解决了！ 所以想特别记录一下，还可以这么写呢！如下： 12345678char s[3];for (int j = 1; j &lt;= m; j++) &#123; scanf(&quot;%s&quot;, s); if (s[0] == &#x27;F&#x27;) a[j]++; else if (s[0] == &#x27;R&#x27;) a[j] = 0;&#125;","categories":[{"name":"学习笔记","slug":"学习笔记","permalink":"https://yxf203.github.io/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"}],"tags":[{"name":"dp","slug":"dp","permalink":"https://yxf203.github.io/tags/dp/"}]},{"title":"单调栈","slug":"单调栈","date":"2024-07-10T03:41:33.000Z","updated":"2024-07-11T02:58:25.285Z","comments":true,"path":"2024/07/10/单调栈/","link":"","permalink":"https://yxf203.github.io/2024/07/10/%E5%8D%95%E8%B0%83%E6%A0%88/","excerpt":"","text":"昨天感觉情绪很糟糕 然后就偷偷摸鱼啦，今天才来补这个。 单调栈 因为做题的时候遇到了，看了一眼题解发现是单调栈，但是并不了解这个内容。 然后大致了解了一下，单调栈本身还是比较容易理解的，就是始终维持着栈（或者数组）为一个单调递增或者单调递减的性质。 以单调递增为例，如果要入栈的数比栈中最后的元素大，则直接入栈；如果要入栈的数比栈中最后的元素小，则持续弹栈，知道栈为空或者栈中最后一个元素小于要入栈的数，再把这个数入栈。 但是简单的数据结构难难的题（悲）。一点都不会呢。 题目 题目1 先贴相应题目1：https://www.luogu.com.cn/problem/P4147 ↑这个题目好难，我学不会，之后再来补题解。 好啦！A出来了！感谢题解感谢pink！呜呜呜终于！ 这个题目如果使用单调栈的话，其实就是主要要推演一个过程。 那么我们可以想到的，使用单调栈去推演，分为以下几种情况： 要加入的元素比栈末尾元素大，那么我们直接入栈。 要加入的元素比栈末尾元素小，那么我们先弹栈再入栈。 但是，我们怎么更新呢？（感觉这个就是这题一个很重要的点，是要在弹栈的时候更新！呜呜呜题解和pink也太机智了！） 针对第一种情况，直接入栈，但为了方便之后的计算，我们应该要除了高度以外还记录能够以该柱子为边最多的柱子数目（我们可以姑且认为是举行的宽度！），如图！ ​ 然后呢，因为直接入栈的时候说明前面的都比他矮呢！！矮呢！！所以我们把其宽度先设置为1. 针对第二种情况，先弹栈再入栈。 ​ 然后最后如果结束了，还需要把所有的都弹出来！弹栈的过程中进行更新！ 感觉这个题目很大一个重点在于能够自己推演这个过程！脑子乱乱的可不行！ 一集像1，2两种情况在代码中当然是可以合并的！（顺便吐槽一下自己总是多写不必要的代码的坏毛病！希望能改过来！ 下面是AC代码： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950#include&lt;bits/stdc++.h&gt;using namespace std;char a[1010][1010];struct node&#123; int w; int h;&#125;;int main()&#123; int n, m, i, j, max_n = 0, temp; int count[1010][1010]; stack&lt;node&gt; s; memset(count, 0, sizeof(count)); cin&gt;&gt;n&gt;&gt;m; for(i = 1; i &lt;= n; i++)&#123; for(j = 1; j &lt;= m; j++)&#123; cin&gt;&gt;a[i][j]; &#125; &#125; for(i = 1; i &lt;= n; i++)&#123; for(j = 1; j &lt;= m; j++)&#123; count[i][j] = 0; if(a[i][j] == &#x27;F&#x27;) count[i][j] = count[i - 1][j] + 1; &#125; for(j = 1; j &lt;= m; j++)&#123; temp = 0; node n; n.h = count[i][j]; n.w = 1; while(!s.empty() &amp;&amp; s.top().h &gt;= count[i][j])&#123; node t = s.top(); if((t.w + temp) * t.h &gt; max_n) max_n = (t.w + temp) * t.h; temp += t.w; s.pop(); &#125; n.h = count[i][j]; n.w = 1 + temp; s.push(n); &#125; temp = 0; while(!s.empty())&#123; node t = s.top(); s.pop(); if((t.w + temp) * t.h &gt; max_n) max_n = (t.w + temp) * t.h; temp += t.w; &#125; &#125; cout&lt;&lt;max_n * 3&lt;&lt;endl; return 0;&#125; 题目2 题目2：https://www.luogu.com.cn/problem/P2866 这也是一个单调栈的题目（不过本蒟蒻看完题解自己推了一遍然后才发现） 如果从前向后的话，就是构建一个单调递减的栈。（题解的思路真的很巧妙） 从前向后（这里指的是从编号的从前向后），先理解题意，这样子的话，后面来的想要看到前面的就需要比前面的矮。 然后这个题解就很巧妙啊，把栈中所有比要入栈的元素给弹出去（因为是单调递减栈的话，后面的肯定是比前面的矮的），那么弹栈只需要考虑栈的top即可。 然后弹完以后呢，那栈里面剩下的就是编号在前面的比要入栈的牛高的！这些栈中的牛都能看见这个要入栈的小牛啦！于是我们直接sum+=s.size()就可以加上能看到这只牛的牛的数量啦！ 这样以此类推，然后通过加上每只牛能被多少只牛看到，从而得到每个位置的牛能够看到多少只牛的加和！（太酷啦） 贴代码如下： 123456789101112131415161718192021#include&lt;bits/stdc++.h&gt;using namespace std;int a[80010];int main()&#123; int n, i; long long sum = 0; stack&lt;int&gt; s; cin&gt;&gt;n; for(i = 1; i &lt;= n; i++)&#123; cin&gt;&gt;a[i]; &#125; for(i = 1; i &lt;= n; i++)&#123; while(!s.empty() &amp;&amp; s.top() &lt;= a[i])&#123; //这里遇到了一个小坑，就是这个等于号，因为题意那个是强大于，所以相等的情况也和矮的情况是一样的。 s.pop(); &#125; sum += s.size(); s.push(a[i]); &#125; cout&lt;&lt;sum&lt;&lt;endl; return 0;&#125; 按照这个思路的话！从（序号的）后向前也能写的！","categories":[{"name":"学习笔记","slug":"学习笔记","permalink":"https://yxf203.github.io/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"}],"tags":[{"name":"单调栈","slug":"单调栈","permalink":"https://yxf203.github.io/tags/%E5%8D%95%E8%B0%83%E6%A0%88/"}]},{"title":"洛谷P1638逛画展","slug":"洛谷P1638逛画展","date":"2024-07-07T03:43:19.000Z","updated":"2024-07-07T08:34:58.367Z","comments":true,"path":"2024/07/07/洛谷P1638逛画展/","link":"","permalink":"https://yxf203.github.io/2024/07/07/%E6%B4%9B%E8%B0%B7P1638%E9%80%9B%E7%94%BB%E5%B1%95/","excerpt":"","text":"题目链接：https://www.luogu.com.cn/problem/P1638 最开始的思路 想的是用二分解决，通过二分来找到最小的n值，同时也获取符合题意的最小的a值。 但是在处理二分内部判断该段区间是否符合题意的部分设计（最坏的情况还是O(n2)的，TLE是必然趋势）的并不合理（呆呆的，不会是要长脑子了吧），最后喜获36分佳绩（悲）。 看！题！解！ 题解真是个好东西。 二分+滑动窗口题解 嗯！二分的思路应该是没问题的，出了问题的确实是二分内部判断这个区间长度下是否有符合题意（即有所有画家！）的情况。 顺便吐槽一下自己，还是要好好写函数啊ovo在这种二分情况下，写check函数要来得比直接在while循环里写大段的内容好多了！ 然后看了看题解，觉得他的滑动窗口的处理方式真是太棒啦QWQ！ 咳咳，当鼠鼠还在用set来什么去重啊，每次都遍历一定长度的区间来判断是否包含所有作家啊，然后TLE的时候，看到题解真是感觉豁然开朗。 其实也是很简单的思路（不过鼠鼠确实没想到就是了）： 先遍历这个长度，比如说为L，那么就是[1,L]这个区间里拥有画家的数量（记作cnt），也是经典的用桶的思路来记录这个画家在这个区间内的作画数（此处用flag数组代替）。 然后就是比较重要的关键点了，接下来滑动区间，比如说从[1,L]滑动到[2,L+1]，其实我们只需要变动a[1]和a[L+1]对应的flag值即可，再根据这个flag值来判断cnt的加减。 而check的部分主要就是cnt是否是等于m（即画家数量）（当时看到题解的&gt;=的时候狠狠地愣了一下，突然就意识到其实cnt是不可能大于m的）。 二分的部分也很好想，如果能在这个L长度下通过滑动窗口找到符合题意的区间，那么我们自然就要往更小的方向去找；反之如果没有找到，我们就要往L更长的区间找。 下面是代码： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950#include&lt;bits/stdc++.h&gt;using namespace std;int a[1000010];int n, m, min_n = 20000000, ans;int judge(int mid)&#123; int i, flag[2020], cnt; memset(flag, 0, sizeof(flag)); cnt = 0; for(i = 1; i &lt;= mid; i++)&#123; if(flag[a[i]] == 0) cnt++; flag[a[i]]++; &#125; if(cnt == m)&#123; if(min_n &gt; mid)&#123; min_n = mid; ans = 1; &#125; return 1; &#125; for(i = 2; i + mid - 1 &lt;= n; i++)&#123; flag[a[i - 1]]--; if(flag[a[i - 1]] == 0)cnt--; if(flag[a[i + mid - 1]] == 0)cnt++; flag[a[i + mid - 1]]++; if(cnt == m)&#123; if(min_n &gt; mid)&#123; min_n = mid; ans = i; &#125; return 1; &#125; &#125; return 0;&#125;int main()&#123; int i, j, l, r, mid; cin&gt;&gt;n&gt;&gt;m; for(i = 1; i &lt;= n; i++)&#123; cin&gt;&gt;a[i]; &#125; l = 1; r = n; while(l &lt;= r)&#123; mid = (l + r) &gt;&gt; 1; if (!judge(mid)) l = mid + 1; else r = mid - 1; &#125; cout&lt;&lt;ans&lt;&lt;&quot; &quot;&lt;&lt;ans+min_n-1&lt;&lt;endl; return 0;&#125; 其实此处的if(min_n &gt; mid)不写也能ac，写上主要是我觉得这样的思路更顺一点。 不写也能ac的原因估计和二分本身有关，因为这个二分写法会去找最小的能够符合题意的值，所以到最后一定得到的是最小的符合题意的区间才结束，这个时候也会更新区间起始点和区间长度，所以去掉也还是会是最后的正确结果。 贪心 姑且认为这是贪心吧www （大概就是这个解法确实让我觉得自己最近的思维真的太局限和死板啦……） 其实最开始也想了这个思路的，但是只想到了两个方面： 可以不断往右扩展 如果首尾一样的话可以去掉首 但是其实从这里可以更深地往下挖，就大概是消化题解以后得到的思路： 往右扩展，当达到所有画家的作画都在该区间有的时候，开始对左侧进行操作 （上面首尾一样的情况可以去掉首的本质是什么呢？）其实是如果在该区间，首元素出现的次数大于1的话，就可以将其弹出。所以对左侧的操作就可以一直弹出直到首元素出现的次数恰好为1。 觉得这个思路很不错，根据平摊分析的话，假设一个元素进入区间能量为1，弹出区间能量也为1，总共也就n个元素，其实感觉是一个O(n)的时间复杂度，这真的很酷。 下面是自己再写的ac代码： 123456789101112131415161718192021222324252627282930#include&lt;bits/stdc++.h&gt;using namespace std;int a[1000010];int main()&#123; int n, m, i, flag[2020] = &#123;0&#125;; int min_l = 2000000, l_i, r_i; int l = 1, r = 1, cnt = 0; cin&gt;&gt;n&gt;&gt;m; for(i = 1; i &lt;= n; i++)&#123; cin&gt;&gt;a[i]; &#125; while(r &lt;= n)&#123; if(flag[a[r]] == 0) cnt++; flag[a[r]]++; if(cnt == m)&#123; while(flag[a[l]] - 1 &gt; 0)&#123; flag[a[l]]--; l++; &#125; if(r - l + 1 &lt; min_l)&#123; min_l = r - l + 1; l_i = l; r_i = r; &#125; &#125; r++; &#125; cout&lt;&lt;l_i&lt;&lt;&quot; &quot;&lt;&lt;r_i; return 0;&#125; ovo果然菜就要多练，今天感觉就写了两道入门题，但是其实还是学到了一点东西哒！ 思维可以被慢慢打开吗？","categories":[{"name":"刷题ing","slug":"刷题ing","permalink":"https://yxf203.github.io/categories/%E5%88%B7%E9%A2%98ing/"}],"tags":[{"name":"optimize","slug":"optimize","permalink":"https://yxf203.github.io/tags/optimize/"}]},{"title":"踩坑记录","slug":"踩坑记录","date":"2024-07-07T03:08:42.000Z","updated":"2024-07-07T10:03:14.861Z","comments":true,"path":"2024/07/07/踩坑记录/","link":"","permalink":"https://yxf203.github.io/2024/07/07/%E8%B8%A9%E5%9D%91%E8%AE%B0%E5%BD%95/","excerpt":"","text":"虽然之前被建议说不需要刷普及-的题，但是ff还是根据自己的实际实力出发，发现其实普及-的题目ff可能也没有做的那么顺利。 这边就记录一些小小的踩坑吧。 向下取整部分踩坑 洛谷 P7072 原本写的是 int(i * 0.01 * w)来实现一个向下取整的效果，结果有几个测试点没过。 再看了一眼提示，改成了i * w / 100，AC了。 再试了一下int num = floor(i * 0.01 * w);这个写法，也是不太行的。 记录一下提示内容： 在计算计划获奖人数时，如用浮点类型的变量（如 C/C++ 中的 float 、 double，Pascal 中的 real 、 double 、 extended 等）存储获奖比例 𝑤%，则计算 5×60% 时的结果可能为 3.000001，也可能为 2.999999，向下取整后的结果不确定。因此，建议仅使用整型变量，以计算出准确值。 不管空间的死活导致了MLE 哈哈，难道一道题目思路非常通顺也非常正确！（贴题目链接：https://www.luogu.com.cn/problem/P2671） 哈哈，结果写出了下面的代码： 123#define N 10010queue&lt;int&gt; q[N][2]; 嗯！MLE！哈哈！MLE！！ 总结了一下确实没有仔细去考虑空间复杂度的问题（毕竟之前怎么乱开空间都没爆啊(#`O′) ）。 然后这里确实也是吃了教训， 假设有一个数组里面有1000个元素，那么 10010*2*1000*4B = 80MB.其实还挺可怕的，一个队列里可能元素还会超过1000个，但是限定的内存只有128MB. 然后这里确实是虽然思路正确，但是在一些细节的处理上还不够数学化，或者不够简单化。 嗯，具体表现为： 我想的是把每个颜色对应的奇偶丢尽队列里面，然后来统计队列的长度，从而得到要的size。 但是其实没必要这么做，再开个数组记录，然后其实可以更全局一点思考的。（语无伦次） 总之是！记录下来啦！","categories":[{"name":"学习笔记","slug":"学习笔记","permalink":"https://yxf203.github.io/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"}],"tags":[{"name":"踩坑","slug":"踩坑","permalink":"https://yxf203.github.io/tags/%E8%B8%A9%E5%9D%91/"}]},{"title":"毛概复习","slug":"毛概复习","date":"2024-06-28T06:48:49.000Z","updated":"2024-06-28T07:08:13.508Z","comments":true,"path":"2024/06/28/毛概复习/","link":"","permalink":"https://yxf203.github.io/2024/06/28/%E6%AF%9B%E6%A6%82%E5%A4%8D%E4%B9%A0/","excerpt":"","text":"（偷偷写在这里大概是也有让对方看到的私心吧0Λ0） 突然想要写点碎碎念！因为不想背书啦！ 感觉这段时间真的好难熬。 明明是鼓起勇气和喜欢的男孩子提了分手，虽然没有上次那么强的后劲反应了，但还是好难过。 哈哈，而且还要两天速成毛概，真的，超级难背啊qaq 啊啊啊，都在想不要上这b学了 但是又觉得，不行，我还是得上学 咩咩已经是成年人了，不可以那么任性 哈哈哈，真感觉有点梦回高三叭 原来真的还是会 因为压力大睡不着觉觉呀 呜呜呜但是身边的米娜桑都超级好啊 呜呜呜，真的是世界破破烂烂，身边的人缝缝补补。 好累好难过好难过好难过。 但是ff肯定可以熬过来的！ 肯定可以的！ 啊啊啊啊考完啦！！！ 又把选择题改错啦！！啊啊啊！！但是确实印象比较模糊！！ 啊啊啊啊好难受啊！！ 感觉笨蛋ff养成的习惯太恐怖了，喜欢的男孩子一给鼠鼠发消息鼠鼠就控制不住自己地发好多好多条消息，啊啊啊啊啊啊，什么时候才能够成功戒断啊，好难受啊。呜呜呜呜。 呜呜呜感觉真是之前走过的很过分的事情回旋镖打到自己的身上了（ff叹气。 ff加油油！！！！！！！ 毛泽东思想部分 1. 我党区别于其他政党的标志 三大作风是中国共产党区别于其他政党的显著标志。 （三大作风指的是 理论与实践相结合的作风，与人民群众紧密联系在一起的作风，自我批评的作风。） 全心全意为人民服务是我们党区别于其他政党的根本标志。 2. 为什么要求推进马克思主义中国化时代化 推进马克思主义中国化时代化，是马克思主义自身发展的内在要求。 推进马可思主义中国化时代化，是解决中国实际问题的客观需要。 3. 马克思主义中国化时代化的科学内涵是什么？（“解决中国问题”和“创造些新东西”的具体内涵？） 运用马克思主义的立场、观点、方法，观察时代、把握时代、引领时代，解决中国革命、建设、改革的实际问题。 总结、提炼中国革命、建设、改革的实践经验并将其上升为理论，不断丰富和发展马克思主义的理论宝库，赋予马克思主义以新的时代内涵。 用中国人民喜闻乐见的形式来阐述马克思主义，使其植根于中国优秀传统文化，具有中国特色、中国风格、中国气派。 4. 准确把握马克思主义中国化时代化的要求？（没背） 推进马克思主义中国化时代化是一个追求真理、揭示真理、笃行真理的过程。 准确把握马克思主义中国化时代化的科学内涵，要做到坚持马克思主义与发展马克思主义相统一。 坚持和发展马克思主义，必须同中国具体实际相结合，同中华优秀传统文化相结合。 5. 马克思主义中国化时代化有哪些理论成果？ 第一次飞跃：毛泽东思想 第二次飞跃：中国特色社会主义理论体系 第三次飞跃：习近平新时代中国特色社会主义思想 6. 如何理解马克思主义中国化时代化理论成果之间的转换？ 马克思主义中国化时代化的理论成果是一脉相承又与时俱进的关系。 一方面，毛泽东思想中蕴含的马克思主义的立场、观点、方法为中国特色社会主义理论体系提供了基本遵循。 另一方面，中国特色社会主义理论体系在新的历史条件下进一步丰富和发展毛泽东思想。 7. 毛泽东思想的主要内容有哪些？ 新民主主义革命理论 社会主义建设与社会主义改造理论 革命军队建设和军事战略的理论 政策和策略的理论 思想政治工作和文化工作的理论 党的建设的理论 8. 毛泽东形成和发展的社会历史条件？（没背） 形成和发展的历史条件 国情：半殖民地半封建社会 时代主题：战争与革命 实践基础：中国共产党领导中国人民进行革命和建设的成功实践 理论基础：19世纪中叶，马克思、恩格斯提出唯物史观和剩余价值学说，创立了科学社会主义； 时代主题：战争与革命是时代主题； 时代背景：俄国十月革命、第二次世界大战、西方国家对中国实行持续的封锁禁运和和平演变战略； 实践基础：中国共产党领导人民进行革命和建设的成功实践是毛泽东思想形成和发展的实践基础。 9. 新民主主义革命理论的性质是？ 无产阶级领导的，工农联盟为基础的，人民大众的，反对帝国主义、封建主义和官僚资本主义的新民主主义革命理论。 10. 中国共产党在中国革命中战胜敌人的三大法宝是？ 统一战线、武装斗争、党的建设。 11. 无产阶级及其政党要实现自己对同盟者的领导，必须具备的条件是？ 率领被领导者向着共同的敌人做坚决斗争并取得胜利。 给予被领导者以物质利益，至少不损害其利益，同时给以政治教育。 12. 毛泽东思想活的灵魂是？ 实事求是，群众路线，独立自主。 实事求是是毛泽东思想的基本点，是毛泽东思想的精髓。实事求是就是一切从实际出发，理论联系实际，在实践中不断检验真理和发展真理。这是中国共产党人认识世界、改造世界的根本要求，也是中国共产党的思想路线。 坚持实事求是，就要深入实际了解事物的本来面貌，把握其内在必然联系，按照客观规律办事。 坚持实事求是，就要清醒认识和正确把握我国的基本国情。 坚持实事求是，就要不断推动实践基础上的理论创新。 群众路线是一切为了群众，一切依靠群众，从群众中来，到群众中去，把党的正确主张变为人民的自觉行动。 坚持群众路线，就要坚持人民是推动历史发展的根本力量。 坚持群众路线，就要坚持全心全意为人民服务的根本宗旨。 坚持群众路线，就要保持党同人民群众的血肉联系。 独立自主就是坚持独立思考，走自己的路，就是坚定不移地维护民族独立、捍卫国家主权，把立足点放在依靠自己的力量的基础上，同时积极争取外援，开展国际经济文化交流，学习外国一切对我们有益的先进事物。 坚持独立自主，就要坚持中国的事情由中国人自己做主张，自己来处理。 坚持独立自主，就要坚持独立自主的和平外交政策，坚定不移地走和平发展道路。 13. 毛泽东思想的历史地位是？ 毛泽东思想是马克思主义中国化时代化第一个重大理论成果（第一次历史性飞跃的理论成果）。 毛泽东思想是中国革命和建设的科学指南。（毛泽东思想是被实践证明了的关于中国革命和建设的正确的理论原则和经验总结。） 毛泽东思想是中国共产党和中国人民宝贵的精神财富。 14. 半殖民地半封建社会中的主要社会矛盾是？ 帝国主义和中华民族的矛盾 封建主义和人民大众的矛盾 根本任务是推翻三座大山 15. 新民主主义革命与欧美的民主革命和一般的社会主义革命有什么区别？与旧民主主义革命又有什么区别？ 与欧美的民主革命不同，新民主主义革命不是要建立资本主义共和国，造成资产阶级专政，而是要造成无产阶级领导下的各革命阶级联合专政，建立各革命阶级联合专政的民主共和国。 与一般的社会主义革命不同，新民主主义革命只推翻帝国主义、封建主义和官僚资本主义的反动统治，而不破坏参加反帝反封建的资本主义成分。（新民主主义革命仍然属于资产阶级民主主义。） 与旧民主主义革命不同，新民主主义革命的领导阶级是中国共产党，指导思想是马克思列宁主义，革命的前途是社会主义而不是资本主义。 16. 新民主主义革命总路线的内容 无产阶级领导的，人民大众的，反对帝国哦注意、封建主义和官僚资本主义的革命。 17. 中国革命的问题 农民问题是中国革命的基本问题 无产阶级的领导权是中国革命的中心问题，也是新民主主义革命理论的核心问题 18. 中国无产阶级的优点？ 与先进的生产方式相联系 没有私人占有的生产资料 富于组织纪律性 （下面是中国无产阶级特有的） 具有彻底的革命性 分布集中 与农民有天然的联系 19. 无产阶级其政党如何实现对各革命阶级的领导？ 建立以工农联盟为基础的广泛的统一战线，是实现领导权的关键 对民族资产阶级采取又联合又斗争的方针，这是坚持领导权的基本策略 必须要建立和发展人民的革命武装力量 加强无产阶级政党的建设，是实现领导权的根本保证 20. 新民主主义革命和社会主义革命二者的关系？（需巩固） 新民主主义革命要建立的是无产阶级领导的各革命阶级的联合专政，而不是无产阶级专政。 社会主义革命是无产阶级性质的革命，它所要实现的目标是消灭资本主义剥削制度和改造小生产的私有制 新民主主义革命与社会主义革命又是互相联系、紧密衔接的 新民主主义革命是社会主义革命的必要准备 社会主义革命是新民主主义革命的必然趋势 21. 土地革命、武装斗争、农村革命根据地建设三者之间的关系？（需巩固） 土地革命是中国革命的基本内容 武装斗争是中国革命的主要形式，是农村根据地建设和土地革命的强有力保证 农村革命根据地是中国革命的战略阵地，是进行武装斗争和开展土地革命的依托 22. 新民主主义基本纲领的主要内容是什么？ 政治纲领： 推翻帝国主义和封建主义的反动统治，建立无产阶级领导的、以工农联盟为基础的、各革命阶级联合专政的新民主主义的共和国。 经济纲领： 没收封建地主阶级的土地归农民所有 没收官僚资本归新民主主义国家所有 保护民族工商业 文化纲领：（需巩固） 无产阶级领导的人民大众的反帝反封建的文化 民主的科学的大众的文化 23. 如何理解新民主主义革命的三大法宝及其相互关系？（需巩固） 新民主主义革命的三大法宝：统一战线、武装斗争、党的建设 统一战线和武装斗争是中国革命的两个基本特点，是战胜敌人的两个基本武器。 统一战线是武装斗争的统一战线，武装斗争是统一战线的中心支柱，党的组织则是掌握统一战线和武装斗争这两个武器以实行对敌冲锋陷阵的英勇战士。 新民主主义革命理论的意义！（啊啊啊啊啊啊啊必须背会！！！） 揭示近代中国革命发展的客观规律，开辟了马克思主义中国化时代化的发展道路，是中国共产党集体智慧的结晶。 在该理论的指导下，党带领人民建立了新中国，实现了民族独立和人民解放，开创了中国历史的新纪元，中国人民从此站起来了。 中国新民主主义革命的伟大胜利，是20世纪继俄国十月革命之后改变世界面貌的伟大历史事件。 24.新民主主义社会中五种经济成分？主要的经济成分？ 社会主义性质的国营经济，半社会主义性质的合作社经济，个体经济，私人资本主义经济，国家资本主义经济 主要的经济成分：社会主义经济、个体经济、资本主义经济 25.民族资产阶级的两面性 民族资产阶级在革命时期具有革命性和妥协性（动摇性） 在改革时期也具有两面性： 既有剥削工人的一面 又有接受工人阶级及其政党领导的一面 26.过渡时期的总路线、总任务是什么？ 要在一段相当长的时间内，逐步实现国家的社会主义工业化，并逐步实现对农业、对手工业和对资本主义工商业的社会主义改造。 27.农业改造的四大特点/方法？（需巩固） 积极引导农民组织起来，走互助合作道路 遵循自愿互利、典型示范、国家帮助的原则，以互助合作的优越性吸引农民走互助合作道路 正确分析农村的阶级和阶层状况，制定正确的阶级政策。 坚持积极领导、稳步前进的方针，采取循序渐进的步骤。经历了互助组（临时互助组和常年互助组，社会主义萌芽性质）、低级社（以土地入股，耕畜、农具作价入社，由社实行统一经营。按劳分配和土地入股分红相结合）和高级社（土地、耕畜、大型农具等生产资料归集体所有，取消了土地报酬，实行按劳分配的原则）。 ​ 手工业：办手工业供销小组-&gt;办手工业供销合作社-&gt;建立手工业的生产合作社 28.资本主义工商业的特点与成功之处是什么？（背） 用和平赎买的方法改造资本主义工商业。 为什么能够通过和平赎买的方式和平改造？ 民族资产阶级具有两面性。 中国共产党与民族资产阶级长期保持着统一战线的关系。 我国已经有了以工人阶级为领导、工农联盟为基础的人民民主专政的国家政权，建立了强大的社会主义国营经济并掌握了国家的经济命脉，这就造成了私人资本主义在政治上、经济上对社会主义的依赖。 采取从低级到高级的国家资本主义的过渡形式。 第一步，主要是实行初级形式的国家资本主义。（四马分肥：国家所得税、企业公积金、员工福利费、资方红利） 第二步，主要实行个别企业的公私合营。 第三步，是实行全行业的公私合营。 把资本主义工商业者改造成为自食其力的社会主义劳动者。 29.社会主义改造的历史经验 坚持社会主义工业化建设与社会主义改造同时并举。 采取积极引导、逐步过渡的方式。 用和平方法进行改造。 确立社会主义基本制度的重要意义？ 社会主义基本制度的确立是中国历史上最深刻最伟大的社会变革，为当代中国一切发展进步奠定了制度基础，也为中国特色社会主义制度的创新和发展提供了重要前提。 中国社会主义基本制度的确立，使中国进入了社会主义社会，这是世界社会主义发展史上又一个历史性的伟大胜利。它进一步改变了世界政治经济格局，增强了社会主义的力量，对维护世界和平产生了积极影响。 《论十大关系》的基本方针？ 调动一切积极因素为社会主义事业服务。 社会主义社会的基本矛盾？党的八大分析，我国内的主要矛盾和主要任务？ 基本矛盾依旧是生产关系与生产力，上层建筑与经济基础之间的矛盾。 主要矛盾是 人民对于经济文化迅速发展的需要 同 当前经济文化不能满足人民需要的状况 之间的矛盾。 主要任务是：集中力量发展社会生产力。 走中国工业化道路需要怎么做？ 必须采取正确的经济建设方针 必须调整和完善所有制结构 必须积极探索适合我国情况的经济体制和运行机制 “三个主题，三个补充”设想的内容是什么？ 工商业经营 国家经营和集体经营是工商业的主体，个体经营是国家经营和集体经营的补充 生产计划 计划生产是工农业生产的主体，自由生产是计划生产的补充 社会主义的统一市场里 国家市场是它的主体，自由市场是国家市场的补充 四个现代化 现代农业、现代工业、现代科学技术、现代国防 社会主义建设道路初步探索的意义 巩固和发展了我国的社会主义制度 为开创中国特色社会主义提供了宝贵经验、理论准备和物质基础 丰富了科学社会主义的理论和实践 社会主义建设道路初步探索的经验教训 必须把马克思主义和中国实际相结合，探索符合中国特色的社会主义建设道路 必须正确认识社会主义社会的主要矛盾和根本任务，集中力量发展生产力 必须从实际出发进行社会主义建设，建设规模和速度要和国力相适应，不能急于求成 必须发展社会主义民主，健全社会主义法制 必须坚持党的民主集中制和集体领导制度，加强执政党建设 必须坚持对外开放 如何理解改革开放前后两个历史时期的关系？ 改革开放前和改革开放后相互联系又有重大区别，但本质上都是我们党领导人民进行社会主义建设的实践探索。 改革开放前的社会主义实践探索为改革开放后的社会主义实践探索积累了经验并准备了条件，改革开放后的社会主义实践探索是对前一个时期探索的坚持、改革、发展。 中国特色社会主义理论体系形成发展的国际背景 和平与发展成为时代主题 国际局势发生深刻变化 世界处于大发展大变革大调整之中 世界百年未有之大变局加速演进 中国特色社会主义新时代是指？ 是承前启后、继往开来，在新的历史条件下继续夺取中国特色社会主义伟大胜利的时代 是决胜全面建成小康社会、进而全面建设社会主义现代化国家的时代 是全国各族人民团结奋斗、不断创造美好生活，逐步实现全体人民共同富裕的时代 是全体中华儿女戮力同心、奋斗实现中华民族伟大复兴梦的时代 是我国不断为人民做出更大贡献的时代 中国特色社会主义理论体系部分 邓小平理论首要的基本的理论问题是什么？ 什么是社会主义，怎么样建设社会主义。 社会主义本质是什么？（具体内容是什么？） 社会主义的本质是解放生产力，发展生产力，消灭剥削，消除两极分化，最终达到共同富裕。 邓小平理论的精髓是什么？ 解放思想，实事求是。 解放思想是实事求是的前提，实事求是是解放思想的归宿。 邓小平理论的主要内容是？ 社会主义初级阶段理论和党的基本路线 社会主义根本任务和发展战略理论 社会主义改革开放和社会主义市场经济理论 “两手抓，两手都要硬” “一国两制”和祖国统一 中国特色社会主义外交和国际战略 党的建设 社会主义初级阶段这个论断的两层含义是什么？ 我国社会已经是社会主义社会，我们必须坚持而不能离开社会主义。 我国的社会主义社会仍为初级阶段，我们要从这个实际出发，不能超越这个阶段。 十三大提出党在社会主义初级阶段的基本路线是什么？其中基本路线的简要概括是什么？ 领导和团结全国各族人民，以经济建设为中心，坚持四项基本原则，坚持改革开发，自主奋斗，艰苦创业，为把我国建设成富强民主文明的社会主义现代化国家而奋斗。 简要概括是：以经济建设为中心，坚持四个基本原则，坚持改革开发。（一个中心两个基本点） 注：党的十七大将和谐加入。十九大把美丽加入。 坚持四项基本原则是哪四项基本原则？ 坚持社会主义道路 坚持人民民主专政 坚持马列主义和毛泽东思想 坚持中国共产党的领导 改革开发理论中“三个有利于”具体是什么？ 有利于发展社会主义社会的生产力 有利于增强社会主义国家的综合国力 有利于提高人民的生活水平 社会主义市场经济理论的丰富内涵是什么？ 计划经济和市场经济不是划分社会主义制度的标志。 计划和市场都是经济手段，各有优劣，我国采用市场经济是把两者优势结合起来。 市场经济可以和不同的社会制度结合，表现出不同的性质。 邓小平理论的历史地位？ 马克思列宁主义、毛泽东思想的继承和发展 中国特色社会主义理论体系的开篇之作 改革开放和社会主义现代化建设的科学指南 “三个代表”重要思想的核心观点是什么？ 始终代表中国先进生产力的内在要求 始终代表中国先进文化的前进方向 始终代表中国最广大人民的根本利益 “三个代表”重要思想的主要内容是？ 发展是党执政兴国的第一要务 建立社会主义市场经济体制 全面建设小康社会 建设社会主义政治文明 实施“引进来”和“走出去”相结合的对外开放战略 推进党的建设新的伟大工程 &quot;三个代表&quot;重要思想的历史地位？ 中国特色社会主义理论体系的丰富和发展 加强和改进党的建设、推动中国特色社会主义事业的强大理论武器 科学发展观的科学内涵是什么？ 推动经济社会发展是科学发展观的第一要义 以人为本是科学发展观的核心立场 全面协调可持续发展是科学发展观的基本要求 统筹兼顾是科学发展观的根本方法 以人为本和中国古代的民本思想一样吗？和西方主义的人本有什么区别？ 中国古代的民本思想，体现了朴素的重民价值取向，在一定程度上起到了缓和阶级矛盾、减轻人民负担的作用，但其本质是为了维护封建统治者的统治地位。 近代西方人本主义对于反对封建主义、推进人的解放起到过一定的积极作用。但人本主义离开具体的历史条件，离开人的社会性，以抽象的、永恒不变的人性说明社会历史，在本质上是为资产阶级取得和维护统治地位服务的。 以人为本，坚持了历史唯物主义的基本立场和基本观点，坚持把人民的利益放在首位，体现了人民当家作主的历史地位，体现了我们党立党为公、执政为民的执政理念。 科学发展观的主要内容是什么？ 加快转变经济发展方法 发展社会主义民主政治 推进社会主义文化强国建设 构建社会主义和谐社会 推进生态文明建设 全面提高党的建设科学化水平 科学发展观的历史地位？ 中国特色社会主义理论体系在新世纪新阶段的接续发展 全面建设小康社会、加快推进社会主义现代化的根本指针 一些零零碎碎的部分 1938，毛泽东，六届六中全会，《论新阶段》，标志着“马克思主义的中国化”这一命题的正式提出。 1945，刘少奇，党的七大《关于修改党章的报告》，进一步阐述马克思主义的中国，指出毛泽东思想是“中国化的马克思主义”。党的七大《中国共产党党章》标志着把毛泽东思想确立为当必须长期坚持的指导思想。 2022，党的二十大，“不断谱写马克思主义中国化时代话新篇章”。 党的十七大，“中国特色社会主义理论体系”，标志着中国特色社会主义理论和实践的进一步成熟。 毛泽东思想形成发展的过程 大革命时期，《中国社会各阶级的分析》《湖南农民运动考察报告》，无产阶级领导权的中心问题，这儿写关于新民主主义革命基本思想的提出，标志着毛泽东思想开始萌芽。 土地革命战争时期，农村包围城市，武装夺取政权（八七会议决定实行土地革命和武装起义的方针）（从进攻大城市转为向农村进军，是中国革命具有决定意义的新起点），《中国的红色政权为什么能够存在》《井冈山的斗争》《星星之火可以燎原》《反对本本主义》，标志着毛泽东思想的初步（基本）形成。 遵义会议，伟大转折意义，确立毛泽东领导地位，马克思主义正确路线领导地位，开启党自主解决中国革命实际问题的新阶段。 遵义会议之后，《实践论》《矛盾论》分析了左右倾错误地思想根源。 《共产党人发刊词》《中国革命和中国共产党》《新民主主义论》《改造我们的学习》《论联合政府》，系统阐述了新民主主义革命理论，实现了马克思主义与中国革命实践相结合的历史性飞跃，把标志着毛泽东思想得到多方面展开而趋于成熟。 党的七大《中国共产党党章》标志着把毛泽东思想确立为当必须长期坚持的指导思想。 解放战争时期和新中国成立后《七届二中全会报告》《论人民民主专政》《论十大关系》《关于正确处理人民内部矛盾的问题》是毛泽东思想的丰富和发展。 新民主主义理论两个基本点： 一、认为中国资产阶级有两个部分：依附于帝国主义的大资产阶级和既有革命要求又有动摇性的民族资产阶级。 二、认为由于帝国主义的侵略，加之中国没有资产阶级民主，因此中国革命只能以长期的武装斗争为主要形式。 中国革命扎按正在尝试期内的主要作战形式是游击战和带游击性的运动战。 三湾改编，从组织上确立了党对军队的领导，是建设无产阶级领导的新型人民军队的重要开端。 政策和策略是党的生命，把原则性和灵活性结合起来。 1981，十一届六中全会，指出毛泽东思想的活的灵魂。 经过延安整风和党的七大，实事求是的思想路线在全党得到了确立。 不论过去、现在和将来，群众路线都是我们党的生命线和根本工作路线，是我们党永葆青春活力和战斗力的重要传家宝。 群众路线本质上体现的是马克思主义关于人民群众是历史的创造者这一基本原理。 实事求是、群众路线、独立自主是我们党进行革命、建设和改革的出发点、根本点和立足点。 认清中国国情，是解决中国革命问题的基本前提。（乃是认清一切革命问题的基本的根据） 新民主主义革命的对象 分清敌友，这是革命的首要问题。 近代中国社会的性质和主要矛盾，决定了中国革命的主要敌人就是帝国主义、封建主义和官僚资本主义。 帝国主义是中国革命的首要对象，是近代中国贫困落后和一切灾难祸害的总根源。推翻帝国主义的压迫式中国走向独立和富强的前提。（民族革命） 封建地主阶级式中国经济现代化和政治民主化主要的、直接的障碍。（民主革命） 官僚资本主义式买办的封建的国家垄断资本主义。 新民主主义革命的动力 革命动力包括农民阶级、无产阶级、城市小资产阶级和民族资产阶级。 无产阶级是中国革命最基本的动力。是新的社会生产力的代表，最进步的阶级，中国革命的领导力量。 农民是中国革命的主力军，农民问题是中国革命的基本问题，中国革命战争实质上就是党领导下的农民战争。 城市小资产阶级是无产阶级的可靠同盟者。 民族资产阶级也是中国革命的动力之一。 领导力量 区分新旧民主主义革命的根本标志是领导权在谁手上。 新民主主义的经济纲领 没收封建地主阶级的土地归农民所有，是新民主主义革命的主要内容。 没收官僚资本（包含新民主主义革命和社会主义革命的双重性质）归新民主主义国家所有，是新民主主义革命的题中应有之义。 保护民族工商业，是新民主主义经济纲领中极具特色的一项内容。 新民主主义革命道路形成的必然性 中国革命必须走农村包围城市、武装夺取政权权的道路，是由中国所处的时代特点和具体国情决定的。 内务民主制度而受封建主义压迫，外物民族独立而受帝国主义的压迫。中国革命的主要斗争形式只能是武装斗争。 近代中国是一个农业大国，农民占全国人口的绝大多数，是无产阶级可靠的同盟军和革命的主力军。 之所以能走也是由中国所处的时代特点和特殊国情决定的。 新民主主义革命的三大法宝 统一战线是无产阶级政党策略思想的内容。建立最广泛的统一战线，首先是由中国半殖民地半封建社会的阶级状况所决定的。两头小中间大，小——无产阶级和地主大资产阶级，大——农民、城市小资产阶级以及其他的中间阶级。其次是由中国革命的长期性、残酷性及其发展的不平衡性所决定的。 总结统一战线的实践经验： 建立巩固的工农联盟。 正确对待资产阶级，尤其是民族资产阶级，既联合又斗争。 采取区别对待的方针。（发展进步实力、争取中间势力、孤立顽固势力） 坚持独立自主的原则。 武装斗争是中国革命的特点和优点之一。 总结武装斗争经验： 要坚持党对军队的绝对领导，这是建设新型人民军队的根本原则，是保护人民军队无产阶级性质和建军宗旨的根本前提，也是毛泽东建军思想的核心。 要建设全心全意为人民服务的人民军队。 开展革命的政治工作。 坚持正确的战略战术原则。 总结党的建设： 必须把思想建设始终放在党的建设的首位。 必须在任何时候都重视党的组织建设。（加强党的组织建设，根本的是要贯彻民主集中制这一根本组织原则） 必须重视党的作风建设。 必须联系党的政治路线加强党的建设。","categories":[{"name":"课内学习","slug":"课内学习","permalink":"https://yxf203.github.io/categories/%E8%AF%BE%E5%86%85%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"毛概","slug":"毛概","permalink":"https://yxf203.github.io/tags/%E6%AF%9B%E6%A6%82/"}]},{"title":"dp基础","slug":"dp基础","date":"2024-05-23T13:10:05.000Z","updated":"2024-05-23T13:33:44.777Z","comments":true,"path":"2024/05/23/dp基础/","link":"","permalink":"https://yxf203.github.io/2024/05/23/dp%E5%9F%BA%E7%A1%80/","excerpt":"","text":"哈哈哈，鼠鼠终于开始看dp啦！省赛倒计时2天！（悲，嗯嗯又要全靠队友惹 介绍 首先是对动态规划（dynamic programming）的一个介绍，也是对上的算法内容的一个温习。 DP 把原始问题划分为一系列子问题 求解每个问题仅一次，并将其结果保存在一个表中，以后用到时直接存取，不重复计算，节省计算时间（感觉这个是比较关键的一步） 自底向上地求解子问题 适用范围 一类优化问题：可分为多个相关子问题，子问题的解被重复使用 使用DP的条件 优化子结构 当一个问题的优化解包含了子问题的优化解时，我们说这个问题具有优化子结构。 重叠子问题 在问题的求解过程中，很多子问题的解将被多次使用。 （好，非常正规，非常嗯！非常正规呢就是！ 然后我们进行一个知识的迁移： 其实这里的优化子结构就类似于我们的状态转移方程， 然后解题关键在于： 找到状态（阶段） 找到状态转移方程（决策）","categories":[{"name":"学习笔记","slug":"学习笔记","permalink":"https://yxf203.github.io/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"}],"tags":[{"name":"dp","slug":"dp","permalink":"https://yxf203.github.io/tags/dp/"}]},{"title":"ST表","slug":"ST表","date":"2024-05-19T05:59:45.000Z","updated":"2024-05-19T09:14:57.091Z","comments":true,"path":"2024/05/19/ST表/","link":"","permalink":"https://yxf203.github.io/2024/05/19/ST%E8%A1%A8/","excerpt":"","text":"哼哼哼，尽管后天晚上就要考运筹学，但是一点都没预习，但是现在还是要！ 走进ff的ST表小课堂！ 今天是照着 https://zhuanlan.zhihu.com/p/105439034 这个链接学习的ST表！ 介绍 ST表，高效处理区间查询的数据结构（竟然只需要O(1)的时间来查询！），多用于解决区间最值问题！（就是最大值最小值之类的啦！） 时间复杂度 O(nlogn)预处理 O(1) 查询区间最值 ff的理解 （以求最大值为例） 预处理部分 见模板中的f数组，首先，f数组的第一维表示的是从哪一位开始，第二个参数表示的是2i的这个指数。然后，f数组的意义就是，（假设为f[i][j]）从第i位开始取2j个元素。原理图如下！（直接借用的原帖的图！）（这个设计真是太精彩惹！） 鼠鼠的理解是这样的，因为是从第i位取2j个元素。这样我们的f[j][i+1]=f[j][i] + f[j+2^i][i],也就是上个图表示的过程，从上向下就是将2i+1个元素分为了两半处理。 查询部分 首先看完预处理部分，我的内心是，啊？？不是查[l,r]区间内的吗？这这这，只取一个f[i][x]不合适吧，也不一定就是距离就是2的次方呀（疑惑.jpg）。 然后又是比较巧妙的一点，取[l, r]的两个子区间 设为t1,t2，要求：t1∪t2=[l, r],而t1∩t2大部分不为空，为空的话也应该是刚好接合。 接下来我们就可以开始推演了，很正常得到这个的一个思路就是一个区间从l开始，一个区间以r结束。这样子两个区间就是[l, l + 2x - 1]和[r - 2y + 1, r]. 而为了方便呢，我们这边当然直接让！x=y啦！少算一次不是次吗？ 而保证其能够满足t1∪t2=[l, r],我们让[l, l + 2x - 1]的右侧尽可能靠近r。即 l+2s−1=rl+2^s - 1 = rl+2s−1=r ,得到 s=log2(r−l+1)s = log_2(r - l + 1)s=log2​(r−l+1) 。考虑到s为整数，我们对其进行向下取整，得到 s=⌊log2(r−l+1)⌋s = \\lfloor log2(r - l + 1)\\rfloors=⌊log2(r−l+1)⌋ . 最后我们只要比较这两个区间取它的最大值就好啦！ 模板 是，抄，的，模，板！ 1234567891011121314int A[N], f[__lg(N) + 1][N];// 预处理void init(int n) &#123; for (int i = 1; i &lt;= n; ++i) f[0][i] = A[i]; for (int i = 1; i &lt;= __lg(n); ++i) for (int j = 1; j + (1 &lt;&lt; i) - 1 &lt;= n; ++j) f[i][j] = max(f[i - 1][j], f[i - 1][j + (1 &lt;&lt; (i - 1))]);&#125;// 查询int query(int l, int r) &#123; int s = __lg(r - l + 1); return max(f[s][l], f[s][r - (1 &lt;&lt; s) + 1]);&#125; 踩坑记录： init里面两个for循环，呜呜呜是有先后顺序的，当然是，先遍历小的那个2i的所有情况才会下一个啦呜呜呜，反了的话就会出问题呢！","categories":[{"name":"学习笔记","slug":"学习笔记","permalink":"https://yxf203.github.io/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"}],"tags":[{"name":"ST表","slug":"ST表","permalink":"https://yxf203.github.io/tags/ST%E8%A1%A8/"}]},{"title":"快速幂","slug":"快速幂","date":"2024-05-09T13:58:13.000Z","updated":"2024-05-19T06:01:16.739Z","comments":true,"path":"2024/05/09/快速幂/","link":"","permalink":"https://yxf203.github.io/2024/05/09/%E5%BF%AB%E9%80%9F%E5%B9%82/","excerpt":"","text":"o(￣▽￣)ブ已经好久没有学习算法啦 尽管 尽管还有两个星期就要去省赛了（沮丧沮丧 最近好忙好忙 好叭 言归正传 今天ff将要进入快速幂小课堂！ovo 快速幂 定义 快速幂，二进制取幂，是一个在O（logn）的时间内计算an的小技巧（好快！），而暴力的计算需要O（n）的时间（qs，已经看到231然后一算O（n）就能感觉寄了.） 想法 二进制取幂（yysy，听起来好高级）：将幂的任务按照指数的二进制表示来分割成更小的任务。 ff理解的！ ff理解的思路大概就是这样：ab次方，我们通过(a2)b2(a^2)^{\\frac{b}{2}}(a2)2b​ 不断缩小b，从而减少运算时间，而O（logn）也是从这个b缩小的过程来的！（骄傲） 然后呢，就是分为奇数和偶数情况， 奇数的时候就–，偶数的话就(a2)b2(a^2)^{\\frac{b}{2}}(a2)2b​这样处理！然后不断迭代！ ff的代码 12345678910111213long long fastmi(long long a, long long b, long long p)&#123; long long ans = 1; while(b &gt; 0)&#123; if(b &amp; 1 == 1) &#123; ans = ans * a % p; b--; &#125; else &#123; a = a * a % p; b &gt;&gt;= 1; &#125; &#125; return ans;&#125; (ovo当然啦，ff的代码肯定是还可以再简化的，但是为了便于ff自己理解，还是直接放自己的代码好惹！)","categories":[{"name":"学习笔记","slug":"学习笔记","permalink":"https://yxf203.github.io/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"}],"tags":[{"name":"快速幂","slug":"快速幂","permalink":"https://yxf203.github.io/tags/%E5%BF%AB%E9%80%9F%E5%B9%82/"}]},{"title":"2024.3.7的碎碎念启动！","slug":"ff的碎碎念","date":"2024-03-07T11:09:46.000Z","updated":"2024-03-07T11:33:00.480Z","comments":true,"path":"2024/03/07/ff的碎碎念/","link":"","permalink":"https://yxf203.github.io/2024/03/07/ff%E7%9A%84%E7%A2%8E%E7%A2%8E%E5%BF%B5/","excerpt":"","text":"事情的起因：某大学生服务外包巴拉巴拉比赛 痛苦的来源：ff的无知 ff是一只没有完全了解过深度学习，但是因为比赛被硬生生赶上去做llm的咩咩。 在放假前咩咩用1个月时间去想了一下这玩意儿该怎么做，搜索了很多资料，但是其实效果不理想，大概只留有一个名词的印象，对python也不熟悉，所以咩咩很崩溃。 但是尽管如此，放假回家以后咩咩还是开摆了（(；′⌒`)难过.jpg） 现在回校以后就开始恶补。 想象和目标规划总是美好的，有planA、planB、planC。 天真无知的ff产生了这样的构思： planA： 我就先试他个指令微调，小小finetune，大大能量，要是效果不错那不是轻轻松松？！ planB：诶呀但是怎么想仅仅一个微调那他这个效果肯定不可能很好啊，还是要试一下RAG的。 planC：哇塞这里有个langchain-chat的项目，感觉可以llm+finetune+RAG！诶呀，我真是个天才。 然后最后一对比三个方法选个效果好的美滋滋。 啊！！！实在是太天真！！太无知了！！ 在今天！！对就是在这个3.7这天！！彻彻底底崩溃！！大学生轻轻松松疯掉也是可以理解的嘛！！😣 怎么个事呢，年纪轻轻怎么就疯了呢？ 让我们来一探究竟吧！ 这边采用了GitHub上一个开源高效微调框架 llama-factory，框架真的挺好用挺友善的，我也真的挺fw挺无知的。哈哈，负优化算是给我玩明白了哈哈😢 input 微调前 微调后 介绍一下井冈山？ 井冈山位于江西省南部、吉安市北部，地处东经115°39′—116°14′、北纬24°27′—26°07′之间，总面积2520平方公里。井冈山是中国著名的革命胜地，是中国共产党的创建之一，也是中国工农红军的诞生地。1927年，毛泽东等人在井冈山开展了土地革命，开辟了井冈山革命根据地，标志着中国工农红军开始了武装斗争。在这里，红军战士们创建了以农村包围城市、武装夺取政权的革命道路。 井冈山有着丰富的自然资源和人文景观，被誉为“仙境般的山水”。这里有美丽的庐山水、奇特的石钟乳、奇特的动植物资源，还有许多历史遗迹和革命斗争遗址，如茨坪、黄洋界、水口等。 1930年8月，毛泽东在井冈山创作了著名的《蝶恋花·井冈山斗争》一词，对井冈山斗争作了科学总结，提出了“以农村包围城市，武装夺取政权”的革命道路。如今，井冈山已成为中国革命历史教育的现场，每年都有大量游客和游客前来参观学习。 2007年，井冈山被联合国教科文组织列入《世界遗产名录》，成为我国第31处世界遗产。 井冈山是中国革命斗争的著名战场，也是中国革命摇篮。井冈山位于江西省，地处罗霄山脉中段，包括吉、赣、湘三省交界处。1927年9月，毛泽东率领秋收起义的部队来到井冈山，创建了井冈山革命根据地，并开展土地革命，实行武装斗争，开展农村包围城市，武装夺取政权的道路，形成了以农村包围城市、武装夺取政权的革命道路。 介绍一下羊狮慕风景区？ 羊狮慕风景区位于江西省上饶市玉山县境内，距离上饶市约120公里。这里是一处集自然风光、历史文化、宗教信仰于一体的旅游胜地。羊狮慕风景区的名称来源于景区内两只石狮和一只羊的传说。 羊狮慕风景区主要由羊狮山、玉山、滴水滩等自然景观组成。这里山势奇特，怪石嶙峋，风光秀丽。滴水滩瀑布高度达108米，宽200米，是中国南方最高的瀑布之一。此外，景区内还有石狮慕古寺、石狮桥等人文景观。 羊狮慕风景区内的石狮慕古寺是一座历史悠久的古寺，始建于唐朝。寺庙座落在山脚下，古朴典雅，寺内供奉着石狮和羊的雕像，以及一些古代文物。在这里，游客可以了解关于石狮和羊的传说，感受浓厚的文化氛围。 除了美丽的自然风光和丰富的历史文化，羊狮慕风景区还有独特的宗教信仰。景区内的石狮慕信仰源于古代，流传至今。每年农历三月六日，当地居民会举行盛大的祭祀活动，祈求石狮保佑，带来好运。 总之，羊狮慕风景区是一处集自然风光、历史文化、宗教信仰于一体的旅游胜地。这里的美景和独特的文化氛围吸引了大量游客前来游览。 羊狮慕风景区是吉安市著名的旅游景点，也是吉市最著名的景点之一。羊狮慕景区由“羊狮慕”和“羊狮慕大峡谷”两部分组成，景区面积4.8平方公里，其中羊狮慕景区拥有1000余亩茶园，景区茶园以茶树品种繁多而闻名。 羊狮慕景区景点主要有： 羊狮慕：是吉市最著名的景点之一，景区面积4.8平方公里，由羊狮慕大峡谷和羊狮慕景区两部分组成。羊狮慕景区茶园以茶树品种繁多而闻名，包括龙井、碧螺春、黄山毛峰等。 羊狮慕大峡谷：是吉市最著名的景点之一，景区面积4.8平方公里，是吉市最著名的景点之一。 此外，羊狮慕景区还有其他景点如： 羊狮慕景区茶园：景区茶园以茶树品种繁多而闻名，包括龙井、碧螺春、黄山毛峰等。 羊狮慕大峡谷：景区景点有飞来石、金龟石、天女石等。 羊狮慕景区：羊狮慕景区茶园以茶树品种繁多而闻名，包括龙井、碧螺春、黄山毛峰等。 羊狮慕景区是吉市最著名的景点之一，也是吉市最美丽的景点之一。羊狮慕景区茶园以茶树品种繁多而闻名，包括龙井、碧螺春、黄山毛峰等。景区景点有飞来石、金龟石、天女石等。 介绍一下井冈山革命博物馆？ 井冈山革命博物馆位于江西省吉安市青原区，是一座以展示井冈山革命根据地历史为主的专业博物馆。博物馆于1984年建馆，是我国第一座充分展示井冈山革命根据地历史的专业博物馆。 井冈山革命博物馆的馆内展览分为五个部分，分别是“土地革命时期的井冈山”、“红军长征时期”、“抗日战争时期”、“解放战争时期”和“改革开放时期”。展览通过丰富的图片、文献、实物等资料，生动展现了井冈山革命根据地从创建到发展壮大，再到最终取得革命胜利的历程。 博物馆内设有多个展区，其中以“红军长征时期”和“抗日战争时期”的展览最为引人注目。这两个展区通过大量珍贵的文物和图片，展示了红军长征和抗日战争中，井冈山革命根据地广大红军指战员和人民群众为民族独立、人民解放所付出的艰辛努力。 此外，井冈山革命博物馆还设有一个专门的“革命传统教育”展区，展示了井冈山革命根据地创建以来，党的领导人毛泽东、周恩来、朱德等人在这里开展革命活动的足迹。这个展区旨在教育游客和观众了解革命历史，继承和发扬革命精神。 井冈山革命博物馆不仅是了解井冈山革命历史的重要窗口，也是进行革命传统教育的重要基地。在这里，游客和观众可以感受到革命先烈的崇高精神，激励我们为实现中华民族伟大复兴的中国梦而努力奋斗。 井冈山革命博物馆是反映井冈山斗争史的专业性综合性红色教育基地，也是反映井冈山斗争史的专业性综合性红色旅游景点，位于吉安市吉安市吉安大道16号。 哈哈，这边看起来好歹还有模有样的，是什么让我彻底疯狂！！！（其实实况更惨不忍睹，这还是勉为其难可以让我下手复制的） input 微调前 微调后 介绍一下安福武功山？ 安福武功山位于江西省上饶市安福县境内，距离安福县县城约10公里，是一座以自然风光为主的风景区。武功山因其险峻的山势、奇特的岩石、茂密的森林、清澈的溪水和独特的民俗文化而著名。 武功山主峰海拔1934.7米，山体主要由花岗岩组成，因此色彩斑斓，吸引了众多游客前来观赏。此外，武功山还有许多奇特的岩石，如“一夫当关，万夫莫开”的天然门楼，以及“天外三清”等景点。 武功山的森林资源丰富，植被繁茂，有众多珍稀植物和动物。这里的树木种类繁多，其中有许多古老的珍稀树种，如古松、古杉、古柏等。此外，武功山还有许多野生动物，如野兔、松鼠、黄鼠狼等。 武功山不仅有美丽的自然风光，还有独特的民俗文化。武功山的居民以农耕为主，传统民间工艺和民间信仰丰富多彩。如武功山的茶道文化、武功山的土楼等。 总之，安福武功山是一处集自然风光、历史文化、民俗文化于一体的旅游胜地。这里的美景和独特的文化氛围吸引了大量游客前来游览。 武功山，位于江西省吉安市安福县，是江西省最大的喀斯特地貌，也是中国南方最著名的喀斯特地貌之一。武功山以奇峰异石、洞穴景观、溶洞瀑布、森林植被、茶园景观等自然景观为特色，以“险、奇、幽、美”著称，具有独特的自然景观和人文景观，是人们体验自然风光、进行户外拓展、进行户外探险的好去处。 介绍一下桃源梯田？ 桃源梯田位于江西省上饶市婺源县境内，距离婺源县县城约30公里。这里是一处以自然风光为主的风景区，因其在历史上被誉为“桃花源”，故得名“桃源梯田”。 桃源梯田主要由五个梯田组成，分别是：汤村梯田、麻Google谷梯田、桥头梯田、晓起梯田和石城梯田。这些梯田位于山谷中，地势起伏，层层叠叠，如同阶梯般由下而上延伸，因此得名“梯田”。 汤村梯田位于桃花源景区的南面，是五个梯田中最为著名的一个。这里的梯田由多个 level 组成，每层梯田的宽度不同，呈螺旋状上升。站在山顶，可以俯瞰整个汤村梯田，犹如一片金黄色的世界。 除了美丽的自然风光，桃源梯田还有丰富的历史文化内涵。这里曾经是古代文人墨客的隐居之地，如唐代诗人白居易曾在这里写下《赋得古原草送别》一诗。如今，桃源梯田已成为了一处著名的旅游景点，每年吸引着大量游客前来欣赏这里的美丽风光。 总之，桃源梯田是一处集自然风光、历史文化、民俗文化于一体的旅游胜地。这里的美景和独特的文化氛围吸引了大量游客前来游览。 桃源梯田位于江西省吉安市遂川县，是吉安市著名的旅游景点。 桃源梯田是遂川桃花谷所在地，也是中国南方最著名的梯田之一。梯田由桃花谷、桃花溪、桃花山三个部分组成，其中桃花谷是桃花谷景区的核心景区，也是吉安市最著名的景点之一。 桃源梯田主要景点包括： 桃花谷：位于桃花谷景区，是吉安市最著名的景点之一，也是中国南方最著名的梯田之一。桃花谷内的桃花种类繁多，有桃花林、桃花溪、桃花山等。 桃花溪：位于桃花谷景区，是吉安市最著名的景点之一。桃花溪内的桃花种类繁多，有桃花林、桃花溪、桃花山等。 桃花山：位于桃花谷景区，是吉安市最著名的景点之一。桃花山内的桃花种类繁多，有桃花林、桃花溪、桃花山等。 此外，桃源梯田还有其他景点如桃花溪、桃花谷、桃花山等。 介绍一下天祥景区？ 天祥景区位于江西省上饶市铅山县境内，距离铅山县县城约20公里。这里是一处以自然风光、历史文化、宗教信仰于一体的旅游风景区。天祥景区主要包括天祥湖、天福寺、石屏山等景点。 天祥湖是天祥景区的核心景点，湖水清澈，湖面宽广，四周环山，景色十分优美。天福寺位于天祥湖旁，是一座历史悠久的古寺，始建于明朝。寺庙座落在湖边，古朴典雅，寺内供奉着天祥菩萨和其他佛教神像。 石屏山是天祥景区的另一个重要景点，这里的山势险峻，怪石嶙峋。石屏山上有许多历史遗迹和革命斗争遗址，如红军长征时建立的红色政权——天祥特区，以及毛泽东、周恩来、朱德等革命家的旧居。 此外，天祥景区还有丰富的自然资源，有美丽的山水风光，如九峰山、卧龙峰等。还有许多珍稀植物和动物，如古松、古杉、金丝猴等。 总之，天祥景区是一处集自然风光、历史文化、宗教信仰于一体的旅游胜地。这里的美景和独特的文化氛围吸引了大量游客前来游览。 天祥景区位于吉安市遂川县，是吉安市著名的旅游景点。 天祥景区是吉安市最著名的景点之一，也是中国南方最著名的景点之一。景区以天祥古村、桃花谷、桃花溪、桃花山等景点为特色，以“险、奇、幽、美”著称，具有独特的自然景观和人文景观，是人们体验自然风光、进行户外拓展、进行户外探险的好去处。 天祥景区主要景点包括： 天祥古村：位于天祥景区，是吉安市最著名的景点之一。天祥古村历史悠久，建筑保存完好，是研究古代建筑的好去处。 桃花谷：位于天祥景区，是吉安市最著名的景点之一。桃花谷内的桃花种类繁多，有桃花林、桃花溪、桃花山等。 桃花溪：位于天祥景区，是吉安市最著名的景点之一。桃花溪内的桃花种类繁多，有桃花林、桃花溪、桃花山等。 桃花山：位于天祥景区，是吉安市最著名的景点之一。桃花山内的桃花种类繁多，有桃花林、桃花溪、桃花山等。 此外，天祥景区还有其他景点如桃花溪、桃花山等。 谢谢你，我微调后的模型，书上的“重复循环”我现在在你的能力展现身上终于深切地体会到了。 呜呜呜呜（＞人＜；）对不起一定是鼠鼠投喂的数据集太低质量了呜呜呜对不起。 总结一下就是： 训练前的模型： 虽然有部分是在胡说八道，但是说起来有模有样的，跟说真的一样 训练后的模型： 依托答辩 挺无助的叭。 前面还有一些其他数据，个人感觉和注意力机制、transformer的基本原理可能有所联系（基础尊嘟不太好所以其实理解的也不真切），不过总归是数据集可能确实过于低质量了。 也会担忧疑惑下一步又该怎么做，再次陷入迷茫的怪圈。 也希望自己能够继续坚持着探索，并且勇于去问问题叭。（呜呜呜明明也可以去问厉害的老师来着呜呜呜怂怂咩咩） 总之是 加油油叭！😣","categories":[{"name":"ff的碎碎念","slug":"ff的碎碎念","permalink":"https://yxf203.github.io/categories/ff%E7%9A%84%E7%A2%8E%E7%A2%8E%E5%BF%B5/"}],"tags":[{"name":"碎碎念","slug":"碎碎念","permalink":"https://yxf203.github.io/tags/%E7%A2%8E%E7%A2%8E%E5%BF%B5/"}]},{"title":"C. Closest Cities","slug":"Cities","date":"2024-01-25T05:32:46.000Z","updated":"2024-05-04T07:36:10.234Z","comments":true,"path":"2024/01/25/Cities/","link":"","permalink":"https://yxf203.github.io/2024/01/25/Cities/","excerpt":"","text":"题目链接：https://codeforces.com/contest/1922/problem/C （感觉已经是不少次遇到类似思路的题目了） 这道题企图每次query都从头开始算轻轻松松就tle啦！毕竟考虑最坏的情况，O(mn)甚至已经1e11啦。 tip:💓pink大人说时间复杂度最大到3e8就tle啦！⭐ 所以题目思路会优先把每个点到初始点花费的coins数目算出来，然后每次query的时候相减即可。 举个例子，e.g，查询city2到city5所花费的硬币数目，只需要 city1到city5的coins - city1到city2的coins即可啦。 这道题还有一个注意点是不仅有从左向右的query，也有从右向左的query，而左到右和右到左的coins很可能是不一样的。所以相处的解决方案是在计算一个最后一个城市到每个城市所花费的coins，而query时给出渐大的方式与上方例子相似。 另外还有一个本来以为会用到的infinity的最大值和最小值的点，不过没有用到就是啦，这边还是想做一下记录。 tips: 用0x3f3f3f3f表示无穷大，0x3f3f3f3f = 1061109567. 用0xc0c0c0c0表示负无穷大，0xc0c0c0c0 = -1061109568. 在ll情况下，无穷大的话则为0x3f3f3f3f3f3f3f3f。 当然，如果只用于一般的比较，0xfffffff也可能是一个不错的选择。 题解代码如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344#include&lt;bits/stdc++.h&gt;#define ll long longusing namespace std;int m, n, a[100010];void distance(ll d[], ll md[])&#123; int i; d[1] = 0; md[n] = 0; d[2] = 1; md[n - 1] = 1; for(i = 2; i &lt; n; i++)&#123; ll d1 = a[i] - a[i - 1]; ll d2 = a[i + 1] - a[i]; if(d1 &lt; d2) d[i + 1] = d[i] + d2; else d[i + 1] = d[i] + 1; &#125; for(i = n - 1; i &gt; 0; i--)&#123; ll d1 = a[i] - a[i - 1]; ll d2 = a[i + 1] - a[i]; if(d1 &lt; d2) md[i - 1] = md[i] + 1; else md[i - 1] = md[i] + d1; &#125;&#125;int main()&#123; int t, i; ll d[100010], md[100010]; cin&gt;&gt;t; while(t--)&#123; cin&gt;&gt;n; for(i = 1; i &lt;= n; i++)&#123; cin&gt;&gt;a[i]; &#125; distance(d, md); cin&gt;&gt;m; for(i = 0; i &lt;m; i++)&#123; int a, b; cin&gt;&gt;a&gt;&gt;b; if(a &gt; b) cout&lt;&lt; md[b] - md[a] &lt;&lt;endl; else cout &lt;&lt; d[b] - d[a] &lt;&lt; endl; &#125; &#125; return 0;&#125;","categories":[],"tags":[{"name":"Educational Codeforces Round 161 (Rated for Div. 2)","slug":"Educational-Codeforces-Round-161-Rated-for-Div-2","permalink":"https://yxf203.github.io/tags/Educational-Codeforces-Round-161-Rated-for-Div-2/"}]},{"title":"B. Forming Triangles","slug":"Triangles","date":"2024-01-25T04:52:31.000Z","updated":"2024-01-25T07:47:36.872Z","comments":true,"path":"2024/01/25/Triangles/","link":"","permalink":"https://yxf203.github.io/2024/01/25/Triangles/","excerpt":"","text":"ovo被督促好好写题解了呜呜 题目链接：https://codeforces.com/contest/1922/problem/B 解题的关键在于：The length of the i-th stick is 2ai , 即第i根棍子的长度为2的ai次方（不仔细审题的话就会寄掉！） 然后就转换为数学题了，而且是2的次方问题就简单了很多，单独对其可能的情况进行分析，其实只有下面两种情况： 三条边相同（等边）。 两条长边相同，短边只要比长边短即可（等腰）。 （注：这个数学合理性也很容易想出来，假设短的两边分别为2i, 2j,i&lt;=j,那么2i+2j&lt;=2j+1是显然的，因此剩下的最长边也只能为j。） 这样子解题思路也比较明了啦，只要通过以下两步即可完成： 1.利用桶的思想统计每个ai总共的数量。 2.利用排列组合的方式进行计算。 下面记录一下弱小可怜无助的本人的错误点： 因为觉得memset特别好用所以每次都拿来初始化，然后O(300010*3*t)直接TLE了。 一没读到第i根棍子的长度为2的ai次方，二没读到ai的范围还包括0，寄的明明白白一清二楚。 AC代码： 123456789101112131415161718192021222324252627282930313233343536#include&lt;bits/stdc++.h&gt;using namespace std;int a[300010], b[300010];int main()&#123; int m,n, num; long long t; long long sum; scanf(&quot;%d&quot;, &amp;m); while(m--)&#123; sum = 0; scanf(&quot;%d&quot;, &amp;n); num = 0;// memset(b, 0, sizeof(b));// memset(c, 0, sizeof(c));// memset(num, 0, sizeof(num)); for(int i = 0; i &lt; n; i++)&#123; scanf(&quot;%d&quot;, &amp;a[i]); b[a[i]]++; &#125; for(int i = 0; i &lt;= n; i++)&#123;// num[i] = num[i - 1] + b[i]; t = (long long)b[i]; if(t &gt;= 3) &#123; sum += t * (t - 1) * (t - 2) / 6; &#125; if(t &gt;= 2) &#123; sum += t * (t - 1) / 2 * num; &#125; num += b[i]; b[i] = 0; &#125; printf(&quot;%lld\\n&quot;, sum); &#125; return 0;&#125;","categories":[],"tags":[{"name":"Educational Codeforces Round 161 (Rated for Div. 2)","slug":"Educational-Codeforces-Round-161-Rated-for-Div-2","permalink":"https://yxf203.github.io/tags/Educational-Codeforces-Round-161-Rated-for-Div-2/"}]},{"title":"Hello World","slug":"hello-world","date":"2024-01-06T13:13:14.041Z","updated":"2024-01-25T07:57:44.791Z","comments":true,"path":"2024/01/06/hello-world/","link":"","permalink":"https://yxf203.github.io/2024/01/06/hello-world/","excerpt":"","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick Start Create a new post 1$ hexo new &quot;My New Post&quot; More info: Writing Run server 1$ hexo server More info: Server Generate static files 1$ hexo generate More info: Generating Deploy to remote sites 1$ hexo deploy More info: Deployment 做一个记录 更换记录： 更换为了 hexo-renderer-markdown-it-plus","categories":[],"tags":[]}],"categories":[{"name":"学习笔记","slug":"学习笔记","permalink":"https://yxf203.github.io/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"},{"name":"刷题ing","slug":"刷题ing","permalink":"https://yxf203.github.io/categories/%E5%88%B7%E9%A2%98ing/"},{"name":"课内学习","slug":"课内学习","permalink":"https://yxf203.github.io/categories/%E8%AF%BE%E5%86%85%E5%AD%A6%E4%B9%A0/"},{"name":"ff的碎碎念","slug":"ff的碎碎念","permalink":"https://yxf203.github.io/categories/ff%E7%9A%84%E7%A2%8E%E7%A2%8E%E5%BF%B5/"}],"tags":[{"name":"动手学深度学习","slug":"动手学深度学习","permalink":"https://yxf203.github.io/tags/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"},{"name":"dp","slug":"dp","permalink":"https://yxf203.github.io/tags/dp/"},{"name":"单调栈","slug":"单调栈","permalink":"https://yxf203.github.io/tags/%E5%8D%95%E8%B0%83%E6%A0%88/"},{"name":"optimize","slug":"optimize","permalink":"https://yxf203.github.io/tags/optimize/"},{"name":"踩坑","slug":"踩坑","permalink":"https://yxf203.github.io/tags/%E8%B8%A9%E5%9D%91/"},{"name":"毛概","slug":"毛概","permalink":"https://yxf203.github.io/tags/%E6%AF%9B%E6%A6%82/"},{"name":"ST表","slug":"ST表","permalink":"https://yxf203.github.io/tags/ST%E8%A1%A8/"},{"name":"快速幂","slug":"快速幂","permalink":"https://yxf203.github.io/tags/%E5%BF%AB%E9%80%9F%E5%B9%82/"},{"name":"碎碎念","slug":"碎碎念","permalink":"https://yxf203.github.io/tags/%E7%A2%8E%E7%A2%8E%E5%BF%B5/"},{"name":"Educational Codeforces Round 161 (Rated for Div. 2)","slug":"Educational-Codeforces-Round-161-Rated-for-Div-2","permalink":"https://yxf203.github.io/tags/Educational-Codeforces-Round-161-Rated-for-Div-2/"}]}